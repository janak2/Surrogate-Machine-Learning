{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":548,"status":"ok","timestamp":1667333381328,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"W0uWtyfxQBzw"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import time\n","\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.utils.data as data_utils\n","from torch import flatten, tensor\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":21433,"status":"ok","timestamp":1667333403308,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"o8NlTKvZQa-M"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/input.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_input \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/input.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m df_output \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/output.csv\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n","File \u001b[1;32mc:\\Users\\daddyj\\Documents\\Batteryze\\backend\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/input.csv'"]}],"source":["df_input = pd.read_csv('input.csv', index_col=0, header=None)\n","df_output = pd.read_csv('output.csv', index_col=0, header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667334020934,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"IVU7cVPfmqWd"},"outputs":[],"source":["INPUT_SIZE = 1200\n","# define training hyperparameters\n","INIT_LR = 1e-3\n","BATCH_SIZE = 256\n","EPOCHS = 1000\n","# define the train and val splits\n","TRAIN_SPLIT = 0.75\n","VAL_SPLIT = 1 - TRAIN_SPLIT\n","# set the device we will be using to train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667334023320,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"cYpHKytyapwD"},"outputs":[],"source":["def create_test_train_data(df_input, df_output):\n","    df_input = df_input.iloc[:,:INPUT_SIZE]\n","    df_output = df_output.iloc[:, -1]\n","\n","    train_x = df_input.sample(frac=0.8)\n","    train_y = df_output.loc[train_x.index]\n","\n","    test_x = df_input.drop(train_x.index)\n","    test_y = df_output.loc[test_x.index]\n","\n","    num_train = len(train_x)\n","    indices = list(range(num_train))\n","    np.random.shuffle(indices)\n","    split = int(np.floor(VAL_SPLIT * num_train))\n","    train_index, valid_index = indices[split:], indices[:split]\n","\n","    valid_x = train_x.iloc[valid_index, :]\n","    valid_y = train_y.iloc[valid_index]\n","\n","    train_x = train_x.iloc[train_index, :]\n","    train_y = train_y.iloc[train_index]\n","\n","    return train_x, train_y, valid_x, valid_y, test_x, test_y"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":656,"status":"ok","timestamp":1667334025354,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"Zk3YOFBic-sk"},"outputs":[],"source":["train_x, train_y, valid_x, valid_y, test_x, test_y = create_test_train_data(df_input, df_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667334025354,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"XUqoytdYHyN6","outputId":"5b4213b4-3595-4618-94de-dd9822846866"},"outputs":[{"name":"stdout","output_type":"stream","text":["27994 9331 9331 46656\n"]}],"source":["print(len(train_x), len(valid_x),len(test_x), len(df_input.index))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667334025354,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"xcnVr03Ile9_"},"outputs":[],"source":["train_target = tensor(train_y.values)\n","train = tensor(train_x.values) \n","train_tensor = data_utils.TensorDataset(train, train_target) \n","train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = BATCH_SIZE, shuffle = True)\n","\n","valid_target = tensor(valid_y.values)\n","valid = tensor(valid_x.values) \n","valid_tensor = data_utils.TensorDataset(valid, valid_target) \n","valid_loader = data_utils.DataLoader(dataset = valid_tensor, batch_size = BATCH_SIZE, shuffle = True)\n","\n","test_target = tensor(test_y.values)\n","test = tensor(test_x.values) \n","test_tensor = data_utils.TensorDataset(test, test_target) \n","test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = BATCH_SIZE, shuffle = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667334157509,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"f0nT5KQsdJcL"},"outputs":[],"source":["class LeNet(nn.Module):\n","\n","\tdef __init__(self):\n","\t\tsuper(LeNet,self).__init__()\n","\t\t# number of hidden nodes in each layer (512)\n","\t\thidden_1 = 512\n","\t\thidden_2 = 512\n","\t\t# linear layer (784 -> hidden_1)\n","\t\tself.fc1 = nn.Linear(INPUT_SIZE, 512)\n","\t\t# linear layer (n_hidden -> hidden_2)\n","\t\tself.fc2 = nn.Linear(512,512)\n","\t\t# linear layer (n_hidden -> 10)\n","\t\tself.fc3 = nn.Linear(512,1)\n","\t\tself.fc4 = nn.Linear(INPUT_SIZE, INPUT_SIZE)\n","\t\tself.act1 = F.relu\n","\n","\t\t# dropout layer (p=0.2)\n","\t\t# dropout prevents overfitting of data\n","\t\tself.droput = nn.Dropout(0.2)\n","    \n","\tdef forward(self, x):\n","\t\t# add hidden layer, with relu activation function\n","\t\tx = F.normalize(x)\n","\t\tx = self.fc4(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc4(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc4(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc4(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc4(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc1(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\tx = self.fc2(x)\n","\t\tx = self.act1(x)\n","\t\tx = self.droput(x)\n","\n","\t\t# add output layer\n","\t\tx = self.fc3(x)\n","\t\tx = flatten(x)\n","\t\treturn x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1667334159482,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"4diLxrz4n4I3","outputId":"f323d787-37e3-4c93-e2a6-c86e84943de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] initializing the LeNet model...\n"]}],"source":["print(\"[INFO] initializing the LeNet model...\")\n","model = LeNet().to(device)\n","# initialize our optimizer and loss function\n","optimizer = torch.optim.Adam(model.parameters(),lr = 0.001, weight_decay=1e-5)\n","criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1531452,"status":"ok","timestamp":1667335691613,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"20pMI8UcoYr_","outputId":"a4980dc7-a4c9-4e4c-b319-b758640220fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] training the network...\n","Epoch: 1 \tTraining Loss: 0.142784 \tValidation Loss: 0.095856\n","Validation loss decreased (inf --> 6.858958).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.101262 \tValidation Loss: 0.095724\n","Validation loss decreased (6.858958 --> 6.528288).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.096808 \tValidation Loss: 0.094575\n","Validation loss decreased (6.528288 --> 6.275376).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.094681 \tValidation Loss: 0.094571\n","Epoch: 5 \tTraining Loss: 0.094720 \tValidation Loss: 0.088160\n","Validation loss decreased (6.275376 --> 6.082520).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.090662 \tValidation Loss: 0.087427\n","Validation loss decreased (6.082520 --> 5.326997).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.087207 \tValidation Loss: 0.085221\n","Epoch: 8 \tTraining Loss: 0.085462 \tValidation Loss: 0.081533\n","Validation loss decreased (5.326997 --> 5.135009).  Saving model ...\n","Epoch: 9 \tTraining Loss: 0.085296 \tValidation Loss: 0.081803\n","Epoch: 10 \tTraining Loss: 0.083748 \tValidation Loss: 0.078771\n","Epoch: 11 \tTraining Loss: 0.083884 \tValidation Loss: 0.078247\n","Epoch: 12 \tTraining Loss: 0.081474 \tValidation Loss: 0.076283\n","Validation loss decreased (5.135009 --> 4.874395).  Saving model ...\n","Epoch: 13 \tTraining Loss: 0.081831 \tValidation Loss: 0.076996\n","Epoch: 14 \tTraining Loss: 0.081304 \tValidation Loss: 0.074768\n","Epoch: 15 \tTraining Loss: 0.080140 \tValidation Loss: 0.082985\n","Validation loss decreased (4.874395 --> 4.555443).  Saving model ...\n","Epoch: 16 \tTraining Loss: 0.080136 \tValidation Loss: 0.075720\n","Epoch: 17 \tTraining Loss: 0.078635 \tValidation Loss: 0.074264\n","Epoch: 18 \tTraining Loss: 0.079638 \tValidation Loss: 0.075109\n","Epoch: 19 \tTraining Loss: 0.079052 \tValidation Loss: 0.075890\n","Epoch: 20 \tTraining Loss: 0.078611 \tValidation Loss: 0.075021\n","Epoch: 21 \tTraining Loss: 0.080067 \tValidation Loss: 0.079145\n","Epoch: 22 \tTraining Loss: 0.080253 \tValidation Loss: 0.075507\n","Epoch: 23 \tTraining Loss: 0.079177 \tValidation Loss: 0.082926\n","Epoch: 24 \tTraining Loss: 0.079793 \tValidation Loss: 0.079673\n","Epoch: 25 \tTraining Loss: 0.079317 \tValidation Loss: 0.074693\n","Epoch: 26 \tTraining Loss: 0.077547 \tValidation Loss: 0.076808\n","Epoch: 27 \tTraining Loss: 0.078561 \tValidation Loss: 0.074217\n","Epoch: 28 \tTraining Loss: 0.078050 \tValidation Loss: 0.075102\n","Epoch: 29 \tTraining Loss: 0.077833 \tValidation Loss: 0.070727\n","Epoch: 30 \tTraining Loss: 0.077839 \tValidation Loss: 0.079278\n","Epoch: 31 \tTraining Loss: 0.077363 \tValidation Loss: 0.076511\n","Epoch: 32 \tTraining Loss: 0.077815 \tValidation Loss: 0.073241\n","Epoch: 33 \tTraining Loss: 0.079610 \tValidation Loss: 0.086984\n","Epoch: 34 \tTraining Loss: 0.079029 \tValidation Loss: 0.078314\n","Epoch: 35 \tTraining Loss: 0.078641 \tValidation Loss: 0.072497\n","Epoch: 36 \tTraining Loss: 0.077960 \tValidation Loss: 0.074210\n","Epoch: 37 \tTraining Loss: 0.076593 \tValidation Loss: 0.073061\n","Epoch: 38 \tTraining Loss: 0.078899 \tValidation Loss: 0.077888\n","Validation loss decreased (4.555443 --> 4.550283).  Saving model ...\n","Epoch: 39 \tTraining Loss: 0.077973 \tValidation Loss: 0.070675\n","Epoch: 40 \tTraining Loss: 0.078536 \tValidation Loss: 0.073117\n","Epoch: 41 \tTraining Loss: 0.077660 \tValidation Loss: 0.077145\n","Epoch: 42 \tTraining Loss: 0.077167 \tValidation Loss: 0.083525\n","Epoch: 43 \tTraining Loss: 0.079888 \tValidation Loss: 0.072784\n","Epoch: 44 \tTraining Loss: 0.076264 \tValidation Loss: 0.072607\n","Epoch: 45 \tTraining Loss: 0.076799 \tValidation Loss: 0.070881\n","Epoch: 46 \tTraining Loss: 0.076714 \tValidation Loss: 0.073134\n","Validation loss decreased (4.550283 --> 4.532496).  Saving model ...\n","Epoch: 47 \tTraining Loss: 0.077689 \tValidation Loss: 0.076718\n","Epoch: 48 \tTraining Loss: 0.076614 \tValidation Loss: 0.069676\n","Epoch: 49 \tTraining Loss: 0.076904 \tValidation Loss: 0.071080\n","Epoch: 50 \tTraining Loss: 0.075776 \tValidation Loss: 0.071671\n","Epoch: 51 \tTraining Loss: 0.074650 \tValidation Loss: 0.070117\n","Epoch: 52 \tTraining Loss: 0.076981 \tValidation Loss: 0.071443\n","Epoch: 53 \tTraining Loss: 0.078517 \tValidation Loss: 0.097092\n","Epoch: 54 \tTraining Loss: 0.081774 \tValidation Loss: 0.071164\n","Epoch: 55 \tTraining Loss: 0.075830 \tValidation Loss: 0.068402\n","Validation loss decreased (4.532496 --> 4.081372).  Saving model ...\n","Epoch: 56 \tTraining Loss: 0.077327 \tValidation Loss: 0.071337\n","Epoch: 57 \tTraining Loss: 0.075807 \tValidation Loss: 0.069772\n","Epoch: 58 \tTraining Loss: 0.075966 \tValidation Loss: 0.071545\n","Epoch: 59 \tTraining Loss: 0.076448 \tValidation Loss: 0.071103\n","Epoch: 60 \tTraining Loss: 0.075112 \tValidation Loss: 0.069823\n","Epoch: 61 \tTraining Loss: 0.075702 \tValidation Loss: 0.071140\n","Epoch: 62 \tTraining Loss: 0.074511 \tValidation Loss: 0.079247\n","Epoch: 63 \tTraining Loss: 0.075843 \tValidation Loss: 0.070052\n","Epoch: 64 \tTraining Loss: 0.075596 \tValidation Loss: 0.081293\n","Validation loss decreased (4.081372 --> 4.053874).  Saving model ...\n","Epoch: 65 \tTraining Loss: 0.074651 \tValidation Loss: 0.068624\n","Epoch: 66 \tTraining Loss: 0.075071 \tValidation Loss: 0.072355\n","Epoch: 67 \tTraining Loss: 0.074168 \tValidation Loss: 0.068045\n","Epoch: 68 \tTraining Loss: 0.074906 \tValidation Loss: 0.070371\n","Epoch: 69 \tTraining Loss: 0.073835 \tValidation Loss: 0.067429\n","Epoch: 70 \tTraining Loss: 0.074229 \tValidation Loss: 0.069212\n","Epoch: 71 \tTraining Loss: 0.074621 \tValidation Loss: 0.072655\n","Epoch: 72 \tTraining Loss: 0.074318 \tValidation Loss: 0.068639\n","Epoch: 73 \tTraining Loss: 0.074518 \tValidation Loss: 0.067957\n","Epoch: 74 \tTraining Loss: 0.074520 \tValidation Loss: 0.072364\n","Epoch: 75 \tTraining Loss: 0.076223 \tValidation Loss: 0.072472\n","Epoch: 76 \tTraining Loss: 0.075497 \tValidation Loss: 0.072971\n","Epoch: 77 \tTraining Loss: 0.075052 \tValidation Loss: 0.076470\n","Epoch: 78 \tTraining Loss: 0.075209 \tValidation Loss: 0.080109\n","Epoch: 79 \tTraining Loss: 0.079922 \tValidation Loss: 0.076924\n","Epoch: 80 \tTraining Loss: 0.075722 \tValidation Loss: 0.071641\n","Epoch: 81 \tTraining Loss: 0.074523 \tValidation Loss: 0.068565\n","Epoch: 82 \tTraining Loss: 0.073445 \tValidation Loss: 0.070159\n","Epoch: 83 \tTraining Loss: 0.074903 \tValidation Loss: 0.072753\n","Epoch: 84 \tTraining Loss: 0.074806 \tValidation Loss: 0.070713\n","Epoch: 85 \tTraining Loss: 0.076189 \tValidation Loss: 0.080077\n","Epoch: 86 \tTraining Loss: 0.078923 \tValidation Loss: 0.069860\n","Epoch: 87 \tTraining Loss: 0.075097 \tValidation Loss: 0.070870\n","Epoch: 88 \tTraining Loss: 0.074603 \tValidation Loss: 0.070022\n","Epoch: 89 \tTraining Loss: 0.075060 \tValidation Loss: 0.071414\n","Epoch: 90 \tTraining Loss: 0.079199 \tValidation Loss: 0.072521\n","Epoch: 91 \tTraining Loss: 0.075454 \tValidation Loss: 0.073076\n","Epoch: 92 \tTraining Loss: 0.074935 \tValidation Loss: 0.072103\n","Epoch: 93 \tTraining Loss: 0.073743 \tValidation Loss: 0.071263\n","Epoch: 94 \tTraining Loss: 0.076291 \tValidation Loss: 0.071018\n","Epoch: 95 \tTraining Loss: 0.076760 \tValidation Loss: 0.071605\n","Epoch: 96 \tTraining Loss: 0.074424 \tValidation Loss: 0.071142\n","Epoch: 97 \tTraining Loss: 0.083231 \tValidation Loss: 0.083285\n","Epoch: 98 \tTraining Loss: 0.080244 \tValidation Loss: 0.074473\n","Epoch: 99 \tTraining Loss: 0.078629 \tValidation Loss: 0.074933\n","Epoch: 100 \tTraining Loss: 0.078278 \tValidation Loss: 0.071799\n","Epoch: 101 \tTraining Loss: 0.076330 \tValidation Loss: 0.068342\n","Epoch: 102 \tTraining Loss: 0.074657 \tValidation Loss: 0.067237\n","Epoch: 103 \tTraining Loss: 0.073076 \tValidation Loss: 0.070216\n","Epoch: 104 \tTraining Loss: 0.075272 \tValidation Loss: 0.068987\n","Epoch: 105 \tTraining Loss: 0.074042 \tValidation Loss: 0.068839\n","Epoch: 106 \tTraining Loss: 0.073860 \tValidation Loss: 0.070835\n","Epoch: 107 \tTraining Loss: 0.075528 \tValidation Loss: 0.070590\n","Epoch: 108 \tTraining Loss: 0.073450 \tValidation Loss: 0.071352\n","Epoch: 109 \tTraining Loss: 0.074062 \tValidation Loss: 0.068090\n","Epoch: 110 \tTraining Loss: 0.073402 \tValidation Loss: 0.072798\n","Epoch: 111 \tTraining Loss: 0.075344 \tValidation Loss: 0.071242\n","Epoch: 112 \tTraining Loss: 0.073836 \tValidation Loss: 0.068900\n","Epoch: 113 \tTraining Loss: 0.074069 \tValidation Loss: 0.068554\n","Epoch: 114 \tTraining Loss: 0.074205 \tValidation Loss: 0.069438\n","Epoch: 115 \tTraining Loss: 0.073920 \tValidation Loss: 0.067818\n","Epoch: 116 \tTraining Loss: 0.074071 \tValidation Loss: 0.069816\n","Epoch: 117 \tTraining Loss: 0.073930 \tValidation Loss: 0.067452\n","Validation loss decreased (4.053874 --> 4.027366).  Saving model ...\n","Epoch: 118 \tTraining Loss: 0.074185 \tValidation Loss: 0.070663\n","Epoch: 119 \tTraining Loss: 0.075832 \tValidation Loss: 0.070623\n","Epoch: 120 \tTraining Loss: 0.075354 \tValidation Loss: 0.072256\n","Epoch: 121 \tTraining Loss: 0.073263 \tValidation Loss: 0.071157\n","Epoch: 122 \tTraining Loss: 0.074519 \tValidation Loss: 0.072761\n","Epoch: 123 \tTraining Loss: 0.075012 \tValidation Loss: 0.067878\n","Epoch: 124 \tTraining Loss: 0.073682 \tValidation Loss: 0.071374\n","Epoch: 125 \tTraining Loss: 0.074135 \tValidation Loss: 0.070049\n","Epoch: 126 \tTraining Loss: 0.074725 \tValidation Loss: 0.068685\n","Epoch: 127 \tTraining Loss: 0.073766 \tValidation Loss: 0.068864\n","Epoch: 128 \tTraining Loss: 0.073799 \tValidation Loss: 0.068821\n","Epoch: 129 \tTraining Loss: 0.074498 \tValidation Loss: 0.068315\n","Epoch: 130 \tTraining Loss: 0.074278 \tValidation Loss: 0.068672\n","Epoch: 131 \tTraining Loss: 0.075449 \tValidation Loss: 0.071348\n","Epoch: 132 \tTraining Loss: 0.076326 \tValidation Loss: 0.070955\n","Epoch: 133 \tTraining Loss: 0.075037 \tValidation Loss: 0.069273\n","Epoch: 134 \tTraining Loss: 0.077683 \tValidation Loss: 0.068494\n","Epoch: 135 \tTraining Loss: 0.073719 \tValidation Loss: 0.072122\n","Epoch: 136 \tTraining Loss: 0.073058 \tValidation Loss: 0.068387\n","Epoch: 137 \tTraining Loss: 0.074727 \tValidation Loss: 0.068269\n","Epoch: 138 \tTraining Loss: 0.074176 \tValidation Loss: 0.068997\n","Epoch: 139 \tTraining Loss: 0.075197 \tValidation Loss: 0.070615\n","Epoch: 140 \tTraining Loss: 0.074182 \tValidation Loss: 0.068443\n","Epoch: 141 \tTraining Loss: 0.073229 \tValidation Loss: 0.068619\n","Epoch: 142 \tTraining Loss: 0.073944 \tValidation Loss: 0.071006\n","Epoch: 143 \tTraining Loss: 0.074666 \tValidation Loss: 0.073391\n","Epoch: 144 \tTraining Loss: 0.074645 \tValidation Loss: 0.068951\n","Epoch: 145 \tTraining Loss: 0.074332 \tValidation Loss: 0.070067\n","Epoch: 146 \tTraining Loss: 0.073383 \tValidation Loss: 0.068966\n","Epoch: 147 \tTraining Loss: 0.074650 \tValidation Loss: 0.071040\n","Epoch: 148 \tTraining Loss: 0.074158 \tValidation Loss: 0.067058\n","Epoch: 149 \tTraining Loss: 0.074166 \tValidation Loss: 0.068802\n","Epoch: 150 \tTraining Loss: 0.074006 \tValidation Loss: 0.070104\n","Epoch: 151 \tTraining Loss: 0.074452 \tValidation Loss: 0.068896\n","Epoch: 152 \tTraining Loss: 0.073579 \tValidation Loss: 0.068414\n","Epoch: 153 \tTraining Loss: 0.073622 \tValidation Loss: 0.071081\n","Epoch: 154 \tTraining Loss: 0.073594 \tValidation Loss: 0.069385\n","Epoch: 155 \tTraining Loss: 0.072512 \tValidation Loss: 0.070472\n","Epoch: 156 \tTraining Loss: 0.073939 \tValidation Loss: 0.070047\n","Epoch: 157 \tTraining Loss: 0.073971 \tValidation Loss: 0.072551\n","Epoch: 158 \tTraining Loss: 0.074498 \tValidation Loss: 0.067736\n","Epoch: 159 \tTraining Loss: 0.075700 \tValidation Loss: 0.071933\n","Epoch: 160 \tTraining Loss: 0.075573 \tValidation Loss: 0.069681\n","Epoch: 161 \tTraining Loss: 0.073723 \tValidation Loss: 0.070262\n","Epoch: 162 \tTraining Loss: 0.077646 \tValidation Loss: 0.074299\n","Epoch: 163 \tTraining Loss: 0.076452 \tValidation Loss: 0.073483\n","Epoch: 164 \tTraining Loss: 0.075986 \tValidation Loss: 0.071873\n","Epoch: 165 \tTraining Loss: 0.074746 \tValidation Loss: 0.070843\n","Epoch: 166 \tTraining Loss: 0.074836 \tValidation Loss: 0.072036\n","Epoch: 167 \tTraining Loss: 0.075743 \tValidation Loss: 0.071000\n","Epoch: 168 \tTraining Loss: 0.076982 \tValidation Loss: 0.071889\n","Epoch: 169 \tTraining Loss: 0.075911 \tValidation Loss: 0.069958\n","Epoch: 170 \tTraining Loss: 0.075774 \tValidation Loss: 0.072047\n","Epoch: 171 \tTraining Loss: 0.075812 \tValidation Loss: 0.070587\n","Epoch: 172 \tTraining Loss: 0.075905 \tValidation Loss: 0.069417\n","Epoch: 173 \tTraining Loss: 0.075404 \tValidation Loss: 0.070598\n","Epoch: 174 \tTraining Loss: 0.077068 \tValidation Loss: 0.076474\n","Epoch: 175 \tTraining Loss: 0.076429 \tValidation Loss: 0.069110\n","Epoch: 176 \tTraining Loss: 0.076025 \tValidation Loss: 0.069634\n","Epoch: 177 \tTraining Loss: 0.078421 \tValidation Loss: 0.076650\n","Epoch: 178 \tTraining Loss: 0.079163 \tValidation Loss: 0.069902\n","Epoch: 179 \tTraining Loss: 0.075655 \tValidation Loss: 0.073166\n","Epoch: 180 \tTraining Loss: 0.077835 \tValidation Loss: 0.073294\n","Epoch: 181 \tTraining Loss: 0.074730 \tValidation Loss: 0.069570\n","Epoch: 182 \tTraining Loss: 0.074677 \tValidation Loss: 0.070601\n","Epoch: 183 \tTraining Loss: 0.078047 \tValidation Loss: 0.074342\n","Epoch: 184 \tTraining Loss: 0.077396 \tValidation Loss: 0.072977\n","Epoch: 185 \tTraining Loss: 0.075836 \tValidation Loss: 0.071928\n","Epoch: 186 \tTraining Loss: 0.074577 \tValidation Loss: 0.076406\n","Validation loss decreased (4.027366 --> 3.864857).  Saving model ...\n","Epoch: 187 \tTraining Loss: 0.075137 \tValidation Loss: 0.069490\n","Epoch: 188 \tTraining Loss: 0.074632 \tValidation Loss: 0.067885\n","Epoch: 189 \tTraining Loss: 0.075322 \tValidation Loss: 0.068921\n","Epoch: 190 \tTraining Loss: 0.075508 \tValidation Loss: 0.069326\n","Epoch: 191 \tTraining Loss: 0.075381 \tValidation Loss: 0.073219\n","Epoch: 192 \tTraining Loss: 0.074400 \tValidation Loss: 0.072593\n","Epoch: 193 \tTraining Loss: 0.074361 \tValidation Loss: 0.067929\n","Epoch: 194 \tTraining Loss: 0.074103 \tValidation Loss: 0.072943\n","Epoch: 195 \tTraining Loss: 0.074189 \tValidation Loss: 0.070910\n","Epoch: 196 \tTraining Loss: 0.073615 \tValidation Loss: 0.067634\n","Epoch: 197 \tTraining Loss: 0.074912 \tValidation Loss: 0.068912\n","Epoch: 198 \tTraining Loss: 0.074068 \tValidation Loss: 0.070273\n","Epoch: 199 \tTraining Loss: 0.075223 \tValidation Loss: 0.074625\n","Epoch: 200 \tTraining Loss: 0.086382 \tValidation Loss: 0.077274\n","Epoch: 201 \tTraining Loss: 0.078731 \tValidation Loss: 0.076195\n","Epoch: 202 \tTraining Loss: 0.078948 \tValidation Loss: 0.074908\n","Epoch: 203 \tTraining Loss: 0.076194 \tValidation Loss: 0.075061\n","Epoch: 204 \tTraining Loss: 0.077346 \tValidation Loss: 0.069914\n","Epoch: 205 \tTraining Loss: 0.075895 \tValidation Loss: 0.072249\n","Epoch: 206 \tTraining Loss: 0.075256 \tValidation Loss: 0.067491\n","Epoch: 207 \tTraining Loss: 0.076296 \tValidation Loss: 0.069841\n","Epoch: 208 \tTraining Loss: 0.076861 \tValidation Loss: 0.076188\n","Epoch: 209 \tTraining Loss: 0.075815 \tValidation Loss: 0.070240\n","Epoch: 210 \tTraining Loss: 0.074666 \tValidation Loss: 0.072013\n","Epoch: 211 \tTraining Loss: 0.075735 \tValidation Loss: 0.072509\n","Epoch: 212 \tTraining Loss: 0.074579 \tValidation Loss: 0.073638\n","Epoch: 213 \tTraining Loss: 0.074213 \tValidation Loss: 0.071023\n","Epoch: 214 \tTraining Loss: 0.075883 \tValidation Loss: 0.071126\n","Epoch: 215 \tTraining Loss: 0.076795 \tValidation Loss: 0.072560\n","Epoch: 216 \tTraining Loss: 0.075990 \tValidation Loss: 0.070309\n","Epoch: 217 \tTraining Loss: 0.074565 \tValidation Loss: 0.075738\n","Epoch: 218 \tTraining Loss: 0.076095 \tValidation Loss: 0.070275\n","Epoch: 219 \tTraining Loss: 0.074456 \tValidation Loss: 0.068474\n","Epoch: 220 \tTraining Loss: 0.075446 \tValidation Loss: 0.075678\n","Epoch: 221 \tTraining Loss: 0.075744 \tValidation Loss: 0.069189\n","Epoch: 222 \tTraining Loss: 0.077204 \tValidation Loss: 0.077596\n","Epoch: 223 \tTraining Loss: 0.079314 \tValidation Loss: 0.072452\n","Epoch: 224 \tTraining Loss: 0.078109 \tValidation Loss: 0.078841\n","Epoch: 225 \tTraining Loss: 0.093830 \tValidation Loss: 0.085084\n","Epoch: 226 \tTraining Loss: 0.086725 \tValidation Loss: 0.077371\n","Epoch: 227 \tTraining Loss: 0.080746 \tValidation Loss: 0.085984\n","Epoch: 228 \tTraining Loss: 0.079387 \tValidation Loss: 0.070230\n","Epoch: 229 \tTraining Loss: 0.076483 \tValidation Loss: 0.069431\n","Epoch: 230 \tTraining Loss: 0.077813 \tValidation Loss: 0.077527\n","Epoch: 231 \tTraining Loss: 0.077162 \tValidation Loss: 0.074033\n","Epoch: 232 \tTraining Loss: 0.076292 \tValidation Loss: 0.070238\n","Epoch: 233 \tTraining Loss: 0.077671 \tValidation Loss: 0.077589\n","Epoch: 234 \tTraining Loss: 0.078976 \tValidation Loss: 0.074587\n","Epoch: 235 \tTraining Loss: 0.076163 \tValidation Loss: 0.070722\n","Epoch: 236 \tTraining Loss: 0.077644 \tValidation Loss: 0.072321\n","Epoch: 237 \tTraining Loss: 0.077080 \tValidation Loss: 0.072770\n","Epoch: 238 \tTraining Loss: 0.077156 \tValidation Loss: 0.085046\n","Epoch: 239 \tTraining Loss: 0.078502 \tValidation Loss: 0.071134\n","Epoch: 240 \tTraining Loss: 0.076526 \tValidation Loss: 0.070624\n","Epoch: 241 \tTraining Loss: 0.081979 \tValidation Loss: 0.082575\n","Epoch: 242 \tTraining Loss: 0.082163 \tValidation Loss: 0.076056\n","Epoch: 243 \tTraining Loss: 0.080668 \tValidation Loss: 0.079432\n","Epoch: 244 \tTraining Loss: 0.088790 \tValidation Loss: 0.081221\n","Epoch: 245 \tTraining Loss: 0.080445 \tValidation Loss: 0.071928\n","Epoch: 246 \tTraining Loss: 0.079225 \tValidation Loss: 0.074113\n","Epoch: 247 \tTraining Loss: 0.076295 \tValidation Loss: 0.068782\n","Epoch: 248 \tTraining Loss: 0.078525 \tValidation Loss: 0.076305\n","Epoch: 249 \tTraining Loss: 0.076712 \tValidation Loss: 0.069666\n","Epoch: 250 \tTraining Loss: 0.078490 \tValidation Loss: 0.076404\n","Epoch: 251 \tTraining Loss: 0.078698 \tValidation Loss: 0.080157\n","Epoch: 252 \tTraining Loss: 0.079282 \tValidation Loss: 0.070310\n","Epoch: 253 \tTraining Loss: 0.077961 \tValidation Loss: 0.074260\n","Epoch: 254 \tTraining Loss: 0.079127 \tValidation Loss: 0.077475\n","Epoch: 255 \tTraining Loss: 0.079836 \tValidation Loss: 0.082335\n","Epoch: 256 \tTraining Loss: 0.084725 \tValidation Loss: 0.086126\n","Epoch: 257 \tTraining Loss: 0.084860 \tValidation Loss: 0.080944\n","Epoch: 258 \tTraining Loss: 0.089198 \tValidation Loss: 0.083384\n","Epoch: 259 \tTraining Loss: 0.085096 \tValidation Loss: 0.081444\n","Epoch: 260 \tTraining Loss: 0.079654 \tValidation Loss: 0.071140\n","Epoch: 261 \tTraining Loss: 0.080976 \tValidation Loss: 0.071653\n","Epoch: 262 \tTraining Loss: 0.076121 \tValidation Loss: 0.070092\n","Epoch: 263 \tTraining Loss: 0.077264 \tValidation Loss: 0.071847\n","Epoch: 264 \tTraining Loss: 0.076726 \tValidation Loss: 0.071516\n","Epoch: 265 \tTraining Loss: 0.078512 \tValidation Loss: 0.073231\n","Epoch: 266 \tTraining Loss: 0.081045 \tValidation Loss: 0.071522\n","Epoch: 267 \tTraining Loss: 0.076180 \tValidation Loss: 0.071531\n","Epoch: 268 \tTraining Loss: 0.077373 \tValidation Loss: 0.077764\n","Epoch: 269 \tTraining Loss: 0.077810 \tValidation Loss: 0.087204\n","Epoch: 270 \tTraining Loss: 0.080065 \tValidation Loss: 0.073845\n","Epoch: 271 \tTraining Loss: 0.077767 \tValidation Loss: 0.071992\n","Epoch: 272 \tTraining Loss: 0.077091 \tValidation Loss: 0.071026\n","Epoch: 273 \tTraining Loss: 0.078231 \tValidation Loss: 0.079168\n","Epoch: 274 \tTraining Loss: 0.079710 \tValidation Loss: 0.075169\n","Epoch: 275 \tTraining Loss: 0.079879 \tValidation Loss: 0.075516\n","Epoch: 276 \tTraining Loss: 0.077503 \tValidation Loss: 0.072402\n","Epoch: 277 \tTraining Loss: 0.078663 \tValidation Loss: 0.071266\n","Epoch: 278 \tTraining Loss: 0.077915 \tValidation Loss: 0.070250\n","Epoch: 279 \tTraining Loss: 0.077461 \tValidation Loss: 0.072979\n","Epoch: 280 \tTraining Loss: 0.076816 \tValidation Loss: 0.073538\n","Epoch: 281 \tTraining Loss: 0.076692 \tValidation Loss: 0.077684\n","Epoch: 282 \tTraining Loss: 0.076271 \tValidation Loss: 0.086109\n","Epoch: 283 \tTraining Loss: 0.077711 \tValidation Loss: 0.074308\n","Epoch: 284 \tTraining Loss: 0.075285 \tValidation Loss: 0.072291\n","Epoch: 285 \tTraining Loss: 0.079485 \tValidation Loss: 0.073481\n","Epoch: 286 \tTraining Loss: 0.075215 \tValidation Loss: 0.073270\n","Epoch: 287 \tTraining Loss: 0.078291 \tValidation Loss: 0.072939\n","Epoch: 288 \tTraining Loss: 0.075801 \tValidation Loss: 0.070077\n","Epoch: 289 \tTraining Loss: 0.079096 \tValidation Loss: 0.076490\n","Epoch: 290 \tTraining Loss: 0.078133 \tValidation Loss: 0.076436\n","Epoch: 291 \tTraining Loss: 0.077172 \tValidation Loss: 0.075565\n","Epoch: 292 \tTraining Loss: 0.076638 \tValidation Loss: 0.072458\n","Epoch: 293 \tTraining Loss: 0.078519 \tValidation Loss: 0.077736\n","Epoch: 294 \tTraining Loss: 0.077981 \tValidation Loss: 0.072870\n","Epoch: 295 \tTraining Loss: 0.075460 \tValidation Loss: 0.070496\n","Epoch: 296 \tTraining Loss: 0.077551 \tValidation Loss: 0.072971\n","Epoch: 297 \tTraining Loss: 0.084375 \tValidation Loss: 0.079506\n","Epoch: 298 \tTraining Loss: 0.081842 \tValidation Loss: 0.078109\n","Epoch: 299 \tTraining Loss: 0.080760 \tValidation Loss: 0.079331\n","Epoch: 300 \tTraining Loss: 0.084516 \tValidation Loss: 0.080922\n","Epoch: 301 \tTraining Loss: 0.085168 \tValidation Loss: 0.081225\n","Epoch: 302 \tTraining Loss: 0.084549 \tValidation Loss: 0.080428\n","Epoch: 303 \tTraining Loss: 0.086884 \tValidation Loss: 0.080740\n","Epoch: 304 \tTraining Loss: 0.083231 \tValidation Loss: 0.081639\n","Epoch: 305 \tTraining Loss: 0.081842 \tValidation Loss: 0.079133\n","Epoch: 306 \tTraining Loss: 0.084105 \tValidation Loss: 0.080801\n","Epoch: 307 \tTraining Loss: 0.080908 \tValidation Loss: 0.079295\n","Epoch: 308 \tTraining Loss: 0.081197 \tValidation Loss: 0.080831\n","Epoch: 309 \tTraining Loss: 0.087827 \tValidation Loss: 0.082055\n","Epoch: 310 \tTraining Loss: 0.084411 \tValidation Loss: 0.074165\n","Epoch: 311 \tTraining Loss: 0.078084 \tValidation Loss: 0.074279\n","Epoch: 312 \tTraining Loss: 0.079251 \tValidation Loss: 0.080114\n","Epoch: 313 \tTraining Loss: 0.079422 \tValidation Loss: 0.075619\n","Epoch: 314 \tTraining Loss: 0.080802 \tValidation Loss: 0.081616\n","Epoch: 315 \tTraining Loss: 0.079593 \tValidation Loss: 0.076116\n","Epoch: 316 \tTraining Loss: 0.078129 \tValidation Loss: 0.076379\n","Epoch: 317 \tTraining Loss: 0.078361 \tValidation Loss: 0.075152\n","Epoch: 318 \tTraining Loss: 0.077690 \tValidation Loss: 0.074387\n","Epoch: 319 \tTraining Loss: 0.076977 \tValidation Loss: 0.072701\n","Epoch: 320 \tTraining Loss: 0.075114 \tValidation Loss: 0.071769\n","Epoch: 321 \tTraining Loss: 0.074174 \tValidation Loss: 0.070110\n","Epoch: 322 \tTraining Loss: 0.077219 \tValidation Loss: 0.074525\n","Epoch: 323 \tTraining Loss: 0.076234 \tValidation Loss: 0.070638\n","Epoch: 324 \tTraining Loss: 0.074788 \tValidation Loss: 0.070454\n","Epoch: 325 \tTraining Loss: 0.076695 \tValidation Loss: 0.075644\n","Epoch: 326 \tTraining Loss: 0.076544 \tValidation Loss: 0.074160\n","Epoch: 327 \tTraining Loss: 0.080722 \tValidation Loss: 0.080771\n","Epoch: 328 \tTraining Loss: 0.081603 \tValidation Loss: 0.080657\n","Epoch: 329 \tTraining Loss: 0.087650 \tValidation Loss: 0.078405\n","Epoch: 330 \tTraining Loss: 0.083433 \tValidation Loss: 0.081796\n","Epoch: 331 \tTraining Loss: 0.080865 \tValidation Loss: 0.073687\n","Epoch: 332 \tTraining Loss: 0.078591 \tValidation Loss: 0.075814\n","Epoch: 333 \tTraining Loss: 0.077393 \tValidation Loss: 0.074082\n","Epoch: 334 \tTraining Loss: 0.077535 \tValidation Loss: 0.074093\n","Epoch: 335 \tTraining Loss: 0.077435 \tValidation Loss: 0.074211\n","Epoch: 336 \tTraining Loss: 0.077813 \tValidation Loss: 0.074218\n","Epoch: 337 \tTraining Loss: 0.077480 \tValidation Loss: 0.071391\n","Epoch: 338 \tTraining Loss: 0.076466 \tValidation Loss: 0.073218\n","Epoch: 339 \tTraining Loss: 0.078320 \tValidation Loss: 0.077600\n","Epoch: 340 \tTraining Loss: 0.082239 \tValidation Loss: 0.081511\n","Epoch: 341 \tTraining Loss: 0.083678 \tValidation Loss: 0.082202\n","Epoch: 342 \tTraining Loss: 0.080380 \tValidation Loss: 0.077859\n","Epoch: 343 \tTraining Loss: 0.079155 \tValidation Loss: 0.074104\n","Epoch: 344 \tTraining Loss: 0.080184 \tValidation Loss: 0.073645\n","Epoch: 345 \tTraining Loss: 0.078319 \tValidation Loss: 0.074113\n","Epoch: 346 \tTraining Loss: 0.087041 \tValidation Loss: 0.078828\n","Epoch: 347 \tTraining Loss: 0.077721 \tValidation Loss: 0.071541\n","Epoch: 348 \tTraining Loss: 0.076973 \tValidation Loss: 0.080670\n","Epoch: 349 \tTraining Loss: 0.081846 \tValidation Loss: 0.074074\n","Epoch: 350 \tTraining Loss: 0.077465 \tValidation Loss: 0.076921\n","Epoch: 351 \tTraining Loss: 0.076707 \tValidation Loss: 0.073668\n","Epoch: 352 \tTraining Loss: 0.076708 \tValidation Loss: 0.072342\n","Epoch: 353 \tTraining Loss: 0.076428 \tValidation Loss: 0.071437\n","Epoch: 354 \tTraining Loss: 0.079143 \tValidation Loss: 0.073514\n","Epoch: 355 \tTraining Loss: 0.076239 \tValidation Loss: 0.071322\n","Epoch: 356 \tTraining Loss: 0.075095 \tValidation Loss: 0.075747\n","Epoch: 357 \tTraining Loss: 0.076013 \tValidation Loss: 0.069913\n","Epoch: 358 \tTraining Loss: 0.076835 \tValidation Loss: 0.075218\n","Epoch: 359 \tTraining Loss: 0.074565 \tValidation Loss: 0.072870\n","Epoch: 360 \tTraining Loss: 0.075086 \tValidation Loss: 0.070823\n","Epoch: 361 \tTraining Loss: 0.075479 \tValidation Loss: 0.076227\n","Epoch: 362 \tTraining Loss: 0.074448 \tValidation Loss: 0.072206\n","Epoch: 363 \tTraining Loss: 0.075537 \tValidation Loss: 0.072648\n","Epoch: 364 \tTraining Loss: 0.074649 \tValidation Loss: 0.073146\n","Epoch: 365 \tTraining Loss: 0.075182 \tValidation Loss: 0.071831\n","Epoch: 366 \tTraining Loss: 0.075987 \tValidation Loss: 0.069935\n","Epoch: 367 \tTraining Loss: 0.076563 \tValidation Loss: 0.076247\n","Epoch: 368 \tTraining Loss: 0.075611 \tValidation Loss: 0.073905\n","Epoch: 369 \tTraining Loss: 0.077505 \tValidation Loss: 0.070167\n","Epoch: 370 \tTraining Loss: 0.076119 \tValidation Loss: 0.070861\n","Epoch: 371 \tTraining Loss: 0.074329 \tValidation Loss: 0.072089\n","Epoch: 372 \tTraining Loss: 0.074212 \tValidation Loss: 0.073623\n","Epoch: 373 \tTraining Loss: 0.075692 \tValidation Loss: 0.079987\n","Epoch: 374 \tTraining Loss: 0.078705 \tValidation Loss: 0.076299\n","Epoch: 375 \tTraining Loss: 0.076027 \tValidation Loss: 0.072951\n","Epoch: 376 \tTraining Loss: 0.075152 \tValidation Loss: 0.073566\n","Epoch: 377 \tTraining Loss: 0.074090 \tValidation Loss: 0.072198\n","Epoch: 378 \tTraining Loss: 0.074152 \tValidation Loss: 0.069076\n","Epoch: 379 \tTraining Loss: 0.074235 \tValidation Loss: 0.070261\n","Epoch: 380 \tTraining Loss: 0.074598 \tValidation Loss: 0.076140\n","Epoch: 381 \tTraining Loss: 0.073255 \tValidation Loss: 0.070756\n","Epoch: 382 \tTraining Loss: 0.074500 \tValidation Loss: 0.070029\n","Epoch: 383 \tTraining Loss: 0.075667 \tValidation Loss: 0.071844\n","Epoch: 384 \tTraining Loss: 0.075750 \tValidation Loss: 0.074437\n","Epoch: 385 \tTraining Loss: 0.074592 \tValidation Loss: 0.072625\n","Epoch: 386 \tTraining Loss: 0.074601 \tValidation Loss: 0.074346\n","Epoch: 387 \tTraining Loss: 0.075032 \tValidation Loss: 0.071918\n","Epoch: 388 \tTraining Loss: 0.075238 \tValidation Loss: 0.072310\n","Epoch: 389 \tTraining Loss: 0.077800 \tValidation Loss: 0.076396\n","Epoch: 390 \tTraining Loss: 0.075393 \tValidation Loss: 0.068866\n","Epoch: 391 \tTraining Loss: 0.077005 \tValidation Loss: 0.074081\n","Epoch: 392 \tTraining Loss: 0.075395 \tValidation Loss: 0.071657\n","Epoch: 393 \tTraining Loss: 0.074094 \tValidation Loss: 0.073224\n","Epoch: 394 \tTraining Loss: 0.074697 \tValidation Loss: 0.069368\n","Epoch: 395 \tTraining Loss: 0.074292 \tValidation Loss: 0.076497\n","Epoch: 396 \tTraining Loss: 0.082340 \tValidation Loss: 0.083891\n","Epoch: 397 \tTraining Loss: 0.084603 \tValidation Loss: 0.078452\n","Epoch: 398 \tTraining Loss: 0.078874 \tValidation Loss: 0.071660\n","Epoch: 399 \tTraining Loss: 0.077662 \tValidation Loss: 0.074942\n","Epoch: 400 \tTraining Loss: 0.079863 \tValidation Loss: 0.080616\n","Epoch: 401 \tTraining Loss: 0.078066 \tValidation Loss: 0.072076\n","Epoch: 402 \tTraining Loss: 0.076526 \tValidation Loss: 0.070406\n","Epoch: 403 \tTraining Loss: 0.080580 \tValidation Loss: 0.080847\n","Epoch: 404 \tTraining Loss: 0.081419 \tValidation Loss: 0.076734\n","Epoch: 405 \tTraining Loss: 0.081685 \tValidation Loss: 0.078622\n","Epoch: 406 \tTraining Loss: 0.078599 \tValidation Loss: 0.075941\n","Epoch: 407 \tTraining Loss: 0.077832 \tValidation Loss: 0.078616\n","Epoch: 408 \tTraining Loss: 0.077302 \tValidation Loss: 0.078359\n","Epoch: 409 \tTraining Loss: 0.079027 \tValidation Loss: 0.075423\n","Epoch: 410 \tTraining Loss: 0.077444 \tValidation Loss: 0.074307\n","Epoch: 411 \tTraining Loss: 0.075504 \tValidation Loss: 0.073753\n","Epoch: 412 \tTraining Loss: 0.075273 \tValidation Loss: 0.071017\n","Epoch: 413 \tTraining Loss: 0.076661 \tValidation Loss: 0.072108\n","Epoch: 414 \tTraining Loss: 0.075296 \tValidation Loss: 0.074839\n","Epoch: 415 \tTraining Loss: 0.077637 \tValidation Loss: 0.072163\n","Epoch: 416 \tTraining Loss: 0.076752 \tValidation Loss: 0.071526\n","Epoch: 417 \tTraining Loss: 0.075543 \tValidation Loss: 0.081954\n","Epoch: 418 \tTraining Loss: 0.075641 \tValidation Loss: 0.073095\n","Epoch: 419 \tTraining Loss: 0.075196 \tValidation Loss: 0.072027\n","Epoch: 420 \tTraining Loss: 0.075371 \tValidation Loss: 0.072801\n","Epoch: 421 \tTraining Loss: 0.075999 \tValidation Loss: 0.071116\n","Epoch: 422 \tTraining Loss: 0.076208 \tValidation Loss: 0.077705\n","Epoch: 423 \tTraining Loss: 0.077795 \tValidation Loss: 0.071245\n","Epoch: 424 \tTraining Loss: 0.075692 \tValidation Loss: 0.071847\n","Epoch: 425 \tTraining Loss: 0.075074 \tValidation Loss: 0.077573\n","Epoch: 426 \tTraining Loss: 0.075757 \tValidation Loss: 0.073269\n","Epoch: 427 \tTraining Loss: 0.075834 \tValidation Loss: 0.072567\n","Epoch: 428 \tTraining Loss: 0.075259 \tValidation Loss: 0.071396\n","Epoch: 429 \tTraining Loss: 0.083617 \tValidation Loss: 0.099179\n","Epoch: 430 \tTraining Loss: 0.090777 \tValidation Loss: 0.081784\n","Epoch: 431 \tTraining Loss: 0.085654 \tValidation Loss: 0.078050\n","Epoch: 432 \tTraining Loss: 0.083127 \tValidation Loss: 0.076257\n","Epoch: 433 \tTraining Loss: 0.079289 \tValidation Loss: 0.073793\n","Epoch: 434 \tTraining Loss: 0.076402 \tValidation Loss: 0.075419\n","Epoch: 435 \tTraining Loss: 0.074716 \tValidation Loss: 0.071081\n","Epoch: 436 \tTraining Loss: 0.075241 \tValidation Loss: 0.073087\n","Epoch: 437 \tTraining Loss: 0.073977 \tValidation Loss: 0.072986\n","Epoch: 438 \tTraining Loss: 0.075348 \tValidation Loss: 0.070554\n","Epoch: 439 \tTraining Loss: 0.074566 \tValidation Loss: 0.080677\n","Epoch: 440 \tTraining Loss: 0.078431 \tValidation Loss: 0.074681\n","Epoch: 441 \tTraining Loss: 0.075737 \tValidation Loss: 0.072337\n","Epoch: 442 \tTraining Loss: 0.072464 \tValidation Loss: 0.068603\n","Epoch: 443 \tTraining Loss: 0.074448 \tValidation Loss: 0.076161\n","Epoch: 444 \tTraining Loss: 0.074459 \tValidation Loss: 0.075408\n","Epoch: 445 \tTraining Loss: 0.076339 \tValidation Loss: 0.080839\n","Epoch: 446 \tTraining Loss: 0.075544 \tValidation Loss: 0.076332\n","Epoch: 447 \tTraining Loss: 0.074034 \tValidation Loss: 0.070613\n","Epoch: 448 \tTraining Loss: 0.073784 \tValidation Loss: 0.070431\n","Epoch: 449 \tTraining Loss: 0.077123 \tValidation Loss: 0.074046\n","Epoch: 450 \tTraining Loss: 0.074662 \tValidation Loss: 0.076358\n","Epoch: 451 \tTraining Loss: 0.076281 \tValidation Loss: 0.076651\n","Epoch: 452 \tTraining Loss: 0.076294 \tValidation Loss: 0.071274\n","Epoch: 453 \tTraining Loss: 0.074069 \tValidation Loss: 0.070600\n","Epoch: 454 \tTraining Loss: 0.078383 \tValidation Loss: 0.080695\n","Epoch: 455 \tTraining Loss: 0.077211 \tValidation Loss: 0.072937\n","Epoch: 456 \tTraining Loss: 0.073555 \tValidation Loss: 0.072871\n","Epoch: 457 \tTraining Loss: 0.075273 \tValidation Loss: 0.074557\n","Epoch: 458 \tTraining Loss: 0.075152 \tValidation Loss: 0.076282\n","Epoch: 459 \tTraining Loss: 0.076378 \tValidation Loss: 0.072087\n","Epoch: 460 \tTraining Loss: 0.073310 \tValidation Loss: 0.072101\n","Epoch: 461 \tTraining Loss: 0.073655 \tValidation Loss: 0.072675\n","Epoch: 462 \tTraining Loss: 0.072523 \tValidation Loss: 0.070785\n","Epoch: 463 \tTraining Loss: 0.072428 \tValidation Loss: 0.070172\n","Epoch: 464 \tTraining Loss: 0.073504 \tValidation Loss: 0.076849\n","Epoch: 465 \tTraining Loss: 0.073414 \tValidation Loss: 0.070302\n","Epoch: 466 \tTraining Loss: 0.073083 \tValidation Loss: 0.070821\n","Epoch: 467 \tTraining Loss: 0.076513 \tValidation Loss: 0.079621\n","Epoch: 468 \tTraining Loss: 0.075475 \tValidation Loss: 0.071418\n","Epoch: 469 \tTraining Loss: 0.074748 \tValidation Loss: 0.075295\n","Epoch: 470 \tTraining Loss: 0.073592 \tValidation Loss: 0.078179\n","Epoch: 471 \tTraining Loss: 0.074367 \tValidation Loss: 0.081377\n","Epoch: 472 \tTraining Loss: 0.077052 \tValidation Loss: 0.080813\n","Epoch: 473 \tTraining Loss: 0.077681 \tValidation Loss: 0.076655\n","Epoch: 474 \tTraining Loss: 0.081415 \tValidation Loss: 0.078832\n","Epoch: 475 \tTraining Loss: 0.082726 \tValidation Loss: 0.076393\n","Epoch: 476 \tTraining Loss: 0.077614 \tValidation Loss: 0.078786\n","Epoch: 477 \tTraining Loss: 0.075109 \tValidation Loss: 0.076994\n","Epoch: 478 \tTraining Loss: 0.080560 \tValidation Loss: 0.073169\n","Epoch: 479 \tTraining Loss: 0.074964 \tValidation Loss: 0.075253\n","Epoch: 480 \tTraining Loss: 0.074792 \tValidation Loss: 0.069221\n","Epoch: 481 \tTraining Loss: 0.077725 \tValidation Loss: 0.077971\n","Epoch: 482 \tTraining Loss: 0.080440 \tValidation Loss: 0.076608\n","Epoch: 483 \tTraining Loss: 0.078010 \tValidation Loss: 0.069439\n","Epoch: 484 \tTraining Loss: 0.075557 \tValidation Loss: 0.074914\n","Epoch: 485 \tTraining Loss: 0.076317 \tValidation Loss: 0.070397\n","Epoch: 486 \tTraining Loss: 0.077072 \tValidation Loss: 0.071947\n","Epoch: 487 \tTraining Loss: 0.076711 \tValidation Loss: 0.071610\n","Epoch: 488 \tTraining Loss: 0.076475 \tValidation Loss: 0.071043\n","Epoch: 489 \tTraining Loss: 0.075118 \tValidation Loss: 0.074396\n","Epoch: 490 \tTraining Loss: 0.076846 \tValidation Loss: 0.069018\n","Epoch: 491 \tTraining Loss: 0.079563 \tValidation Loss: 0.075383\n","Epoch: 492 \tTraining Loss: 0.079508 \tValidation Loss: 0.072808\n","Epoch: 493 \tTraining Loss: 0.077912 \tValidation Loss: 0.079906\n","Epoch: 494 \tTraining Loss: 0.078745 \tValidation Loss: 0.073079\n","Epoch: 495 \tTraining Loss: 0.076356 \tValidation Loss: 0.078787\n","Epoch: 496 \tTraining Loss: 0.077517 \tValidation Loss: 0.071118\n","Epoch: 497 \tTraining Loss: 0.074618 \tValidation Loss: 0.066682\n","Epoch: 498 \tTraining Loss: 0.074936 \tValidation Loss: 0.069482\n","Epoch: 499 \tTraining Loss: 0.074573 \tValidation Loss: 0.067325\n","Epoch: 500 \tTraining Loss: 0.074283 \tValidation Loss: 0.071517\n","Epoch: 501 \tTraining Loss: 0.075799 \tValidation Loss: 0.075218\n","Epoch: 502 \tTraining Loss: 0.074658 \tValidation Loss: 0.070727\n","Epoch: 503 \tTraining Loss: 0.071956 \tValidation Loss: 0.068616\n","Epoch: 504 \tTraining Loss: 0.074684 \tValidation Loss: 0.072729\n","Epoch: 505 \tTraining Loss: 0.073903 \tValidation Loss: 0.068856\n","Epoch: 506 \tTraining Loss: 0.072895 \tValidation Loss: 0.068241\n","Epoch: 507 \tTraining Loss: 0.073468 \tValidation Loss: 0.069749\n","Epoch: 508 \tTraining Loss: 0.075563 \tValidation Loss: 0.069715\n","Validation loss decreased (3.864857 --> 3.844197).  Saving model ...\n","Epoch: 509 \tTraining Loss: 0.073543 \tValidation Loss: 0.068749\n","Epoch: 510 \tTraining Loss: 0.074985 \tValidation Loss: 0.071337\n","Epoch: 511 \tTraining Loss: 0.074503 \tValidation Loss: 0.068092\n","Epoch: 512 \tTraining Loss: 0.072597 \tValidation Loss: 0.069833\n","Epoch: 513 \tTraining Loss: 0.074508 \tValidation Loss: 0.077214\n","Epoch: 514 \tTraining Loss: 0.073567 \tValidation Loss: 0.069975\n","Epoch: 515 \tTraining Loss: 0.074014 \tValidation Loss: 0.070453\n","Epoch: 516 \tTraining Loss: 0.072968 \tValidation Loss: 0.065052\n","Epoch: 517 \tTraining Loss: 0.075806 \tValidation Loss: 0.072905\n","Epoch: 518 \tTraining Loss: 0.075332 \tValidation Loss: 0.072518\n","Epoch: 519 \tTraining Loss: 0.076449 \tValidation Loss: 0.070621\n","Epoch: 520 \tTraining Loss: 0.074136 \tValidation Loss: 0.067617\n","Epoch: 521 \tTraining Loss: 0.073101 \tValidation Loss: 0.070023\n","Epoch: 522 \tTraining Loss: 0.071677 \tValidation Loss: 0.073925\n","Epoch: 523 \tTraining Loss: 0.076036 \tValidation Loss: 0.068946\n","Epoch: 524 \tTraining Loss: 0.071970 \tValidation Loss: 0.072527\n","Epoch: 525 \tTraining Loss: 0.072708 \tValidation Loss: 0.066482\n","Epoch: 526 \tTraining Loss: 0.073597 \tValidation Loss: 0.067730\n","Epoch: 527 \tTraining Loss: 0.074740 \tValidation Loss: 0.069134\n","Epoch: 528 \tTraining Loss: 0.074868 \tValidation Loss: 0.069310\n","Epoch: 529 \tTraining Loss: 0.073748 \tValidation Loss: 0.069753\n","Epoch: 530 \tTraining Loss: 0.074278 \tValidation Loss: 0.075293\n","Epoch: 531 \tTraining Loss: 0.074811 \tValidation Loss: 0.070553\n","Epoch: 532 \tTraining Loss: 0.071863 \tValidation Loss: 0.071275\n","Epoch: 533 \tTraining Loss: 0.071730 \tValidation Loss: 0.067977\n","Epoch: 534 \tTraining Loss: 0.072309 \tValidation Loss: 0.070240\n","Epoch: 535 \tTraining Loss: 0.075466 \tValidation Loss: 0.078361\n","Epoch: 536 \tTraining Loss: 0.075973 \tValidation Loss: 0.073096\n","Epoch: 537 \tTraining Loss: 0.075193 \tValidation Loss: 0.069979\n","Epoch: 538 \tTraining Loss: 0.074412 \tValidation Loss: 0.067366\n","Epoch: 539 \tTraining Loss: 0.073725 \tValidation Loss: 0.067665\n","Epoch: 540 \tTraining Loss: 0.074814 \tValidation Loss: 0.072164\n","Epoch: 541 \tTraining Loss: 0.074498 \tValidation Loss: 0.070764\n","Epoch: 542 \tTraining Loss: 0.073306 \tValidation Loss: 0.068870\n","Epoch: 543 \tTraining Loss: 0.073285 \tValidation Loss: 0.069579\n","Epoch: 544 \tTraining Loss: 0.072222 \tValidation Loss: 0.069404\n","Epoch: 545 \tTraining Loss: 0.072947 \tValidation Loss: 0.070136\n","Epoch: 546 \tTraining Loss: 0.076038 \tValidation Loss: 0.069505\n","Epoch: 547 \tTraining Loss: 0.074248 \tValidation Loss: 0.070154\n","Epoch: 548 \tTraining Loss: 0.072732 \tValidation Loss: 0.069659\n","Epoch: 549 \tTraining Loss: 0.072746 \tValidation Loss: 0.072231\n","Epoch: 550 \tTraining Loss: 0.074192 \tValidation Loss: 0.072203\n","Epoch: 551 \tTraining Loss: 0.077975 \tValidation Loss: 0.074789\n","Epoch: 552 \tTraining Loss: 0.074637 \tValidation Loss: 0.071580\n","Epoch: 553 \tTraining Loss: 0.072625 \tValidation Loss: 0.068872\n","Epoch: 554 \tTraining Loss: 0.071378 \tValidation Loss: 0.065960\n","Epoch: 555 \tTraining Loss: 0.071201 \tValidation Loss: 0.069718\n","Epoch: 556 \tTraining Loss: 0.072730 \tValidation Loss: 0.074029\n","Epoch: 557 \tTraining Loss: 0.074851 \tValidation Loss: 0.071077\n","Epoch: 558 \tTraining Loss: 0.088607 \tValidation Loss: 0.078327\n","Epoch: 559 \tTraining Loss: 0.079349 \tValidation Loss: 0.076989\n","Epoch: 560 \tTraining Loss: 0.074565 \tValidation Loss: 0.070820\n","Epoch: 561 \tTraining Loss: 0.071914 \tValidation Loss: 0.071904\n","Epoch: 562 \tTraining Loss: 0.082732 \tValidation Loss: 0.081221\n","Epoch: 563 \tTraining Loss: 0.080045 \tValidation Loss: 0.075780\n","Epoch: 564 \tTraining Loss: 0.076411 \tValidation Loss: 0.071777\n","Epoch: 565 \tTraining Loss: 0.076926 \tValidation Loss: 0.073769\n","Epoch: 566 \tTraining Loss: 0.075545 \tValidation Loss: 0.071764\n","Epoch: 567 \tTraining Loss: 0.077138 \tValidation Loss: 0.072766\n","Epoch: 568 \tTraining Loss: 0.074474 \tValidation Loss: 0.074781\n","Epoch: 569 \tTraining Loss: 0.074064 \tValidation Loss: 0.081129\n","Epoch: 570 \tTraining Loss: 0.079398 \tValidation Loss: 0.076517\n","Epoch: 571 \tTraining Loss: 0.079299 \tValidation Loss: 0.091716\n","Epoch: 572 \tTraining Loss: 0.082758 \tValidation Loss: 0.075674\n","Epoch: 573 \tTraining Loss: 0.075566 \tValidation Loss: 0.071694\n","Epoch: 574 \tTraining Loss: 0.086821 \tValidation Loss: 0.081627\n","Epoch: 575 \tTraining Loss: 0.079145 \tValidation Loss: 0.072847\n","Epoch: 576 \tTraining Loss: 0.075215 \tValidation Loss: 0.073057\n","Epoch: 577 \tTraining Loss: 0.074316 \tValidation Loss: 0.071906\n","Epoch: 578 \tTraining Loss: 0.076772 \tValidation Loss: 0.077853\n","Epoch: 579 \tTraining Loss: 0.073827 \tValidation Loss: 0.071952\n","Epoch: 580 \tTraining Loss: 0.073739 \tValidation Loss: 0.073105\n","Epoch: 581 \tTraining Loss: 0.072932 \tValidation Loss: 0.071540\n","Epoch: 582 \tTraining Loss: 0.072739 \tValidation Loss: 0.071023\n","Epoch: 583 \tTraining Loss: 0.073008 \tValidation Loss: 0.069813\n","Epoch: 584 \tTraining Loss: 0.072885 \tValidation Loss: 0.074118\n","Epoch: 585 \tTraining Loss: 0.076291 \tValidation Loss: 0.073797\n","Epoch: 586 \tTraining Loss: 0.072748 \tValidation Loss: 0.071175\n","Epoch: 587 \tTraining Loss: 0.074327 \tValidation Loss: 0.074074\n","Epoch: 588 \tTraining Loss: 0.074579 \tValidation Loss: 0.071762\n","Epoch: 589 \tTraining Loss: 0.071897 \tValidation Loss: 0.071897\n","Epoch: 590 \tTraining Loss: 0.074041 \tValidation Loss: 0.070364\n","Epoch: 591 \tTraining Loss: 0.073576 \tValidation Loss: 0.068056\n","Epoch: 592 \tTraining Loss: 0.072807 \tValidation Loss: 0.072264\n","Epoch: 593 \tTraining Loss: 0.073074 \tValidation Loss: 0.070136\n","Epoch: 594 \tTraining Loss: 0.074509 \tValidation Loss: 0.076123\n","Epoch: 595 \tTraining Loss: 0.073069 \tValidation Loss: 0.072188\n","Epoch: 596 \tTraining Loss: 0.071384 \tValidation Loss: 0.075553\n","Epoch: 597 \tTraining Loss: 0.073126 \tValidation Loss: 0.071760\n","Epoch: 598 \tTraining Loss: 0.075219 \tValidation Loss: 0.070165\n","Epoch: 599 \tTraining Loss: 0.071264 \tValidation Loss: 0.067965\n","Epoch: 600 \tTraining Loss: 0.072361 \tValidation Loss: 0.070564\n","Epoch: 601 \tTraining Loss: 0.072394 \tValidation Loss: 0.074036\n","Epoch: 602 \tTraining Loss: 0.072796 \tValidation Loss: 0.071069\n","Epoch: 603 \tTraining Loss: 0.071841 \tValidation Loss: 0.071265\n","Epoch: 604 \tTraining Loss: 0.072788 \tValidation Loss: 0.069003\n","Epoch: 605 \tTraining Loss: 0.072530 \tValidation Loss: 0.077566\n","Epoch: 606 \tTraining Loss: 0.075677 \tValidation Loss: 0.082235\n","Epoch: 607 \tTraining Loss: 0.077478 \tValidation Loss: 0.085903\n","Epoch: 608 \tTraining Loss: 0.080150 \tValidation Loss: 0.073452\n","Epoch: 609 \tTraining Loss: 0.075612 \tValidation Loss: 0.070335\n","Epoch: 610 \tTraining Loss: 0.073514 \tValidation Loss: 0.071952\n","Epoch: 611 \tTraining Loss: 0.072658 \tValidation Loss: 0.075831\n","Epoch: 612 \tTraining Loss: 0.072010 \tValidation Loss: 0.074330\n","Epoch: 613 \tTraining Loss: 0.075243 \tValidation Loss: 0.085260\n","Epoch: 614 \tTraining Loss: 0.076882 \tValidation Loss: 0.071839\n","Epoch: 615 \tTraining Loss: 0.074255 \tValidation Loss: 0.071629\n","Epoch: 616 \tTraining Loss: 0.074512 \tValidation Loss: 0.071693\n","Epoch: 617 \tTraining Loss: 0.073456 \tValidation Loss: 0.078033\n","Epoch: 618 \tTraining Loss: 0.076664 \tValidation Loss: 0.078544\n","Epoch: 619 \tTraining Loss: 0.074576 \tValidation Loss: 0.069249\n","Epoch: 620 \tTraining Loss: 0.073879 \tValidation Loss: 0.075042\n","Epoch: 621 \tTraining Loss: 0.072613 \tValidation Loss: 0.076373\n","Epoch: 622 \tTraining Loss: 0.074682 \tValidation Loss: 0.069278\n","Epoch: 623 \tTraining Loss: 0.075017 \tValidation Loss: 0.079121\n","Epoch: 624 \tTraining Loss: 0.073038 \tValidation Loss: 0.071722\n","Epoch: 625 \tTraining Loss: 0.072765 \tValidation Loss: 0.073942\n","Epoch: 626 \tTraining Loss: 0.071849 \tValidation Loss: 0.075939\n","Epoch: 627 \tTraining Loss: 0.075317 \tValidation Loss: 0.072556\n","Epoch: 628 \tTraining Loss: 0.072025 \tValidation Loss: 0.072089\n","Epoch: 629 \tTraining Loss: 0.072817 \tValidation Loss: 0.072085\n","Epoch: 630 \tTraining Loss: 0.072825 \tValidation Loss: 0.068640\n","Epoch: 631 \tTraining Loss: 0.073734 \tValidation Loss: 0.073634\n","Epoch: 632 \tTraining Loss: 0.072956 \tValidation Loss: 0.071430\n","Epoch: 633 \tTraining Loss: 0.071867 \tValidation Loss: 0.068853\n","Epoch: 634 \tTraining Loss: 0.072804 \tValidation Loss: 0.074934\n","Epoch: 635 \tTraining Loss: 0.072716 \tValidation Loss: 0.073557\n","Epoch: 636 \tTraining Loss: 0.075107 \tValidation Loss: 0.075534\n","Epoch: 637 \tTraining Loss: 0.072448 \tValidation Loss: 0.073364\n","Epoch: 638 \tTraining Loss: 0.074506 \tValidation Loss: 0.076849\n","Epoch: 639 \tTraining Loss: 0.077357 \tValidation Loss: 0.074655\n","Epoch: 640 \tTraining Loss: 0.077040 \tValidation Loss: 0.076499\n","Epoch: 641 \tTraining Loss: 0.077168 \tValidation Loss: 0.078525\n","Epoch: 642 \tTraining Loss: 0.077244 \tValidation Loss: 0.077650\n","Epoch: 643 \tTraining Loss: 0.075335 \tValidation Loss: 0.074726\n","Epoch: 644 \tTraining Loss: 0.074844 \tValidation Loss: 0.080098\n","Epoch: 645 \tTraining Loss: 0.074982 \tValidation Loss: 0.075144\n","Epoch: 646 \tTraining Loss: 0.073972 \tValidation Loss: 0.075842\n","Epoch: 647 \tTraining Loss: 0.075542 \tValidation Loss: 0.072079\n","Epoch: 648 \tTraining Loss: 0.074354 \tValidation Loss: 0.076408\n","Epoch: 649 \tTraining Loss: 0.073627 \tValidation Loss: 0.077900\n","Epoch: 650 \tTraining Loss: 0.073049 \tValidation Loss: 0.068581\n","Epoch: 651 \tTraining Loss: 0.072036 \tValidation Loss: 0.069845\n","Epoch: 652 \tTraining Loss: 0.072194 \tValidation Loss: 0.073534\n","Epoch: 653 \tTraining Loss: 0.071305 \tValidation Loss: 0.070020\n","Epoch: 654 \tTraining Loss: 0.072785 \tValidation Loss: 0.074678\n","Epoch: 655 \tTraining Loss: 0.072273 \tValidation Loss: 0.071085\n","Epoch: 656 \tTraining Loss: 0.071950 \tValidation Loss: 0.069018\n","Epoch: 657 \tTraining Loss: 0.072661 \tValidation Loss: 0.076148\n","Epoch: 658 \tTraining Loss: 0.071690 \tValidation Loss: 0.070743\n","Epoch: 659 \tTraining Loss: 0.072643 \tValidation Loss: 0.074115\n","Epoch: 660 \tTraining Loss: 0.072743 \tValidation Loss: 0.075108\n","Epoch: 661 \tTraining Loss: 0.071953 \tValidation Loss: 0.073464\n","Epoch: 662 \tTraining Loss: 0.072065 \tValidation Loss: 0.073888\n","Epoch: 663 \tTraining Loss: 0.073113 \tValidation Loss: 0.073958\n","Epoch: 664 \tTraining Loss: 0.072822 \tValidation Loss: 0.074949\n","Epoch: 665 \tTraining Loss: 0.072907 \tValidation Loss: 0.076838\n","Epoch: 666 \tTraining Loss: 0.073265 \tValidation Loss: 0.076119\n","Epoch: 667 \tTraining Loss: 0.072542 \tValidation Loss: 0.073209\n","Epoch: 668 \tTraining Loss: 0.071270 \tValidation Loss: 0.069369\n","Epoch: 669 \tTraining Loss: 0.073376 \tValidation Loss: 0.071159\n","Epoch: 670 \tTraining Loss: 0.074381 \tValidation Loss: 0.078994\n","Epoch: 671 \tTraining Loss: 0.076171 \tValidation Loss: 0.090004\n","Epoch: 672 \tTraining Loss: 0.075627 \tValidation Loss: 0.072483\n","Epoch: 673 \tTraining Loss: 0.074273 \tValidation Loss: 0.077208\n","Epoch: 674 \tTraining Loss: 0.073878 \tValidation Loss: 0.078771\n","Epoch: 675 \tTraining Loss: 0.072954 \tValidation Loss: 0.077598\n","Epoch: 676 \tTraining Loss: 0.074240 \tValidation Loss: 0.068811\n","Epoch: 677 \tTraining Loss: 0.071856 \tValidation Loss: 0.075140\n","Epoch: 678 \tTraining Loss: 0.072368 \tValidation Loss: 0.073736\n","Epoch: 679 \tTraining Loss: 0.073158 \tValidation Loss: 0.069365\n","Epoch: 680 \tTraining Loss: 0.074103 \tValidation Loss: 0.081792\n","Epoch: 681 \tTraining Loss: 0.082787 \tValidation Loss: 0.078012\n","Epoch: 682 \tTraining Loss: 0.079425 \tValidation Loss: 0.074820\n","Epoch: 683 \tTraining Loss: 0.076660 \tValidation Loss: 0.080310\n","Epoch: 684 \tTraining Loss: 0.075721 \tValidation Loss: 0.075759\n","Epoch: 685 \tTraining Loss: 0.074345 \tValidation Loss: 0.074949\n","Epoch: 686 \tTraining Loss: 0.074498 \tValidation Loss: 0.073294\n","Epoch: 687 \tTraining Loss: 0.074328 \tValidation Loss: 0.074705\n","Epoch: 688 \tTraining Loss: 0.073229 \tValidation Loss: 0.076579\n","Epoch: 689 \tTraining Loss: 0.074543 \tValidation Loss: 0.076140\n","Epoch: 690 \tTraining Loss: 0.072274 \tValidation Loss: 0.072366\n","Epoch: 691 \tTraining Loss: 0.073862 \tValidation Loss: 0.073441\n","Epoch: 692 \tTraining Loss: 0.073063 \tValidation Loss: 0.077217\n","Epoch: 693 \tTraining Loss: 0.074564 \tValidation Loss: 0.075634\n","Epoch: 694 \tTraining Loss: 0.074371 \tValidation Loss: 0.075980\n","Epoch: 695 \tTraining Loss: 0.072415 \tValidation Loss: 0.075250\n","Epoch: 696 \tTraining Loss: 0.071021 \tValidation Loss: 0.076680\n","Epoch: 697 \tTraining Loss: 0.071663 \tValidation Loss: 0.069736\n","Epoch: 698 \tTraining Loss: 0.071469 \tValidation Loss: 0.071625\n","Epoch: 699 \tTraining Loss: 0.070915 \tValidation Loss: 0.073531\n","Epoch: 700 \tTraining Loss: 0.070754 \tValidation Loss: 0.074280\n","Epoch: 701 \tTraining Loss: 0.072677 \tValidation Loss: 0.071433\n","Epoch: 702 \tTraining Loss: 0.071460 \tValidation Loss: 0.074623\n","Epoch: 703 \tTraining Loss: 0.072963 \tValidation Loss: 0.076038\n","Epoch: 704 \tTraining Loss: 0.070806 \tValidation Loss: 0.075885\n","Epoch: 705 \tTraining Loss: 0.070922 \tValidation Loss: 0.083192\n","Epoch: 706 \tTraining Loss: 0.071252 \tValidation Loss: 0.081314\n","Epoch: 707 \tTraining Loss: 0.072397 \tValidation Loss: 0.078183\n","Epoch: 708 \tTraining Loss: 0.075677 \tValidation Loss: 0.081679\n","Epoch: 709 \tTraining Loss: 0.079293 \tValidation Loss: 0.074460\n","Epoch: 710 \tTraining Loss: 0.074405 \tValidation Loss: 0.071407\n","Epoch: 711 \tTraining Loss: 0.071844 \tValidation Loss: 0.073724\n","Epoch: 712 \tTraining Loss: 0.071436 \tValidation Loss: 0.072076\n","Epoch: 713 \tTraining Loss: 0.072030 \tValidation Loss: 0.078133\n","Epoch: 714 \tTraining Loss: 0.071867 \tValidation Loss: 0.076465\n","Epoch: 715 \tTraining Loss: 0.071822 \tValidation Loss: 0.072114\n","Epoch: 716 \tTraining Loss: 0.072089 \tValidation Loss: 0.073591\n","Epoch: 717 \tTraining Loss: 0.071079 \tValidation Loss: 0.072184\n","Epoch: 718 \tTraining Loss: 0.070438 \tValidation Loss: 0.073259\n","Epoch: 719 \tTraining Loss: 0.070888 \tValidation Loss: 0.074108\n","Epoch: 720 \tTraining Loss: 0.070006 \tValidation Loss: 0.075076\n","Epoch: 721 \tTraining Loss: 0.070107 \tValidation Loss: 0.069157\n","Epoch: 722 \tTraining Loss: 0.072888 \tValidation Loss: 0.077523\n","Epoch: 723 \tTraining Loss: 0.073495 \tValidation Loss: 0.074769\n","Epoch: 724 \tTraining Loss: 0.071608 \tValidation Loss: 0.075938\n","Epoch: 725 \tTraining Loss: 0.071018 \tValidation Loss: 0.075130\n","Epoch: 726 \tTraining Loss: 0.069632 \tValidation Loss: 0.068215\n","Epoch: 727 \tTraining Loss: 0.071225 \tValidation Loss: 0.066462\n","Epoch: 728 \tTraining Loss: 0.070849 \tValidation Loss: 0.071837\n","Epoch: 729 \tTraining Loss: 0.070697 \tValidation Loss: 0.067142\n","Epoch: 730 \tTraining Loss: 0.071665 \tValidation Loss: 0.071671\n","Epoch: 731 \tTraining Loss: 0.072356 \tValidation Loss: 0.073297\n","Epoch: 732 \tTraining Loss: 0.069872 \tValidation Loss: 0.071576\n","Epoch: 733 \tTraining Loss: 0.070821 \tValidation Loss: 0.073091\n","Epoch: 734 \tTraining Loss: 0.072592 \tValidation Loss: 0.072982\n","Epoch: 735 \tTraining Loss: 0.071157 \tValidation Loss: 0.078126\n","Epoch: 736 \tTraining Loss: 0.070634 \tValidation Loss: 0.073856\n","Epoch: 737 \tTraining Loss: 0.070799 \tValidation Loss: 0.080192\n","Epoch: 738 \tTraining Loss: 0.069969 \tValidation Loss: 0.076401\n","Epoch: 739 \tTraining Loss: 0.070367 \tValidation Loss: 0.076120\n","Epoch: 740 \tTraining Loss: 0.071664 \tValidation Loss: 0.076304\n","Epoch: 741 \tTraining Loss: 0.069965 \tValidation Loss: 0.072024\n","Epoch: 742 \tTraining Loss: 0.072085 \tValidation Loss: 0.077510\n","Epoch: 743 \tTraining Loss: 0.070547 \tValidation Loss: 0.078545\n","Epoch: 744 \tTraining Loss: 0.070721 \tValidation Loss: 0.071315\n","Epoch: 745 \tTraining Loss: 0.070027 \tValidation Loss: 0.077564\n","Epoch: 746 \tTraining Loss: 0.071400 \tValidation Loss: 0.077785\n","Epoch: 747 \tTraining Loss: 0.070384 \tValidation Loss: 0.076428\n","Epoch: 748 \tTraining Loss: 0.070114 \tValidation Loss: 0.074363\n","Epoch: 749 \tTraining Loss: 0.073384 \tValidation Loss: 0.084049\n","Epoch: 750 \tTraining Loss: 0.071339 \tValidation Loss: 0.080481\n","Epoch: 751 \tTraining Loss: 0.072206 \tValidation Loss: 0.076631\n","Epoch: 752 \tTraining Loss: 0.072939 \tValidation Loss: 0.068280\n","Epoch: 753 \tTraining Loss: 0.070365 \tValidation Loss: 0.074202\n","Epoch: 754 \tTraining Loss: 0.070315 \tValidation Loss: 0.076704\n","Epoch: 755 \tTraining Loss: 0.070190 \tValidation Loss: 0.074269\n","Epoch: 756 \tTraining Loss: 0.074807 \tValidation Loss: 0.080318\n","Epoch: 757 \tTraining Loss: 0.078615 \tValidation Loss: 0.072203\n","Epoch: 758 \tTraining Loss: 0.072538 \tValidation Loss: 0.073959\n","Epoch: 759 \tTraining Loss: 0.070630 \tValidation Loss: 0.075667\n","Epoch: 760 \tTraining Loss: 0.070339 \tValidation Loss: 0.074273\n","Epoch: 761 \tTraining Loss: 0.072124 \tValidation Loss: 0.078370\n","Epoch: 762 \tTraining Loss: 0.070605 \tValidation Loss: 0.079276\n","Epoch: 763 \tTraining Loss: 0.072251 \tValidation Loss: 0.076282\n","Epoch: 764 \tTraining Loss: 0.069603 \tValidation Loss: 0.073677\n","Epoch: 765 \tTraining Loss: 0.069878 \tValidation Loss: 0.074673\n","Epoch: 766 \tTraining Loss: 0.070124 \tValidation Loss: 0.073925\n","Epoch: 767 \tTraining Loss: 0.069887 \tValidation Loss: 0.079734\n","Epoch: 768 \tTraining Loss: 0.070253 \tValidation Loss: 0.080771\n","Epoch: 769 \tTraining Loss: 0.069684 \tValidation Loss: 0.068984\n","Epoch: 770 \tTraining Loss: 0.069773 \tValidation Loss: 0.080643\n","Epoch: 771 \tTraining Loss: 0.071857 \tValidation Loss: 0.071957\n","Epoch: 772 \tTraining Loss: 0.069529 \tValidation Loss: 0.078340\n","Epoch: 773 \tTraining Loss: 0.069047 \tValidation Loss: 0.075639\n","Epoch: 774 \tTraining Loss: 0.069148 \tValidation Loss: 0.079109\n","Epoch: 775 \tTraining Loss: 0.070781 \tValidation Loss: 0.077874\n","Epoch: 776 \tTraining Loss: 0.070151 \tValidation Loss: 0.073652\n","Epoch: 777 \tTraining Loss: 0.069708 \tValidation Loss: 0.068213\n","Epoch: 778 \tTraining Loss: 0.069538 \tValidation Loss: 0.069163\n","Epoch: 779 \tTraining Loss: 0.071367 \tValidation Loss: 0.082227\n","Epoch: 780 \tTraining Loss: 0.070260 \tValidation Loss: 0.073535\n","Epoch: 781 \tTraining Loss: 0.069348 \tValidation Loss: 0.076825\n","Epoch: 782 \tTraining Loss: 0.068499 \tValidation Loss: 0.080947\n","Epoch: 783 \tTraining Loss: 0.073983 \tValidation Loss: 0.072639\n","Epoch: 784 \tTraining Loss: 0.069975 \tValidation Loss: 0.078821\n","Epoch: 785 \tTraining Loss: 0.069553 \tValidation Loss: 0.085129\n","Epoch: 786 \tTraining Loss: 0.068863 \tValidation Loss: 0.074194\n","Epoch: 787 \tTraining Loss: 0.068517 \tValidation Loss: 0.080150\n","Epoch: 788 \tTraining Loss: 0.071885 \tValidation Loss: 0.078519\n","Epoch: 789 \tTraining Loss: 0.070476 \tValidation Loss: 0.078066\n","Epoch: 790 \tTraining Loss: 0.070073 \tValidation Loss: 0.078335\n","Epoch: 791 \tTraining Loss: 0.069740 \tValidation Loss: 0.078401\n","Epoch: 792 \tTraining Loss: 0.069216 \tValidation Loss: 0.075072\n","Epoch: 793 \tTraining Loss: 0.071648 \tValidation Loss: 0.075494\n","Epoch: 794 \tTraining Loss: 0.070842 \tValidation Loss: 0.071319\n","Epoch: 795 \tTraining Loss: 0.071020 \tValidation Loss: 0.086477\n","Epoch: 796 \tTraining Loss: 0.071642 \tValidation Loss: 0.075717\n","Epoch: 797 \tTraining Loss: 0.069676 \tValidation Loss: 0.071447\n","Epoch: 798 \tTraining Loss: 0.070985 \tValidation Loss: 0.075149\n","Epoch: 799 \tTraining Loss: 0.070496 \tValidation Loss: 0.074091\n","Epoch: 800 \tTraining Loss: 0.071522 \tValidation Loss: 0.081046\n","Epoch: 801 \tTraining Loss: 0.071289 \tValidation Loss: 0.080727\n","Epoch: 802 \tTraining Loss: 0.069360 \tValidation Loss: 0.074447\n","Epoch: 803 \tTraining Loss: 0.068899 \tValidation Loss: 0.077428\n","Epoch: 804 \tTraining Loss: 0.070306 \tValidation Loss: 0.073673\n","Epoch: 805 \tTraining Loss: 0.068722 \tValidation Loss: 0.077982\n","Epoch: 806 \tTraining Loss: 0.070672 \tValidation Loss: 0.074380\n","Epoch: 807 \tTraining Loss: 0.071369 \tValidation Loss: 0.077478\n","Epoch: 808 \tTraining Loss: 0.069698 \tValidation Loss: 0.075569\n","Epoch: 809 \tTraining Loss: 0.070166 \tValidation Loss: 0.068173\n","Epoch: 810 \tTraining Loss: 0.070585 \tValidation Loss: 0.083768\n","Epoch: 811 \tTraining Loss: 0.074387 \tValidation Loss: 0.092415\n","Epoch: 812 \tTraining Loss: 0.092966 \tValidation Loss: 0.090672\n","Epoch: 813 \tTraining Loss: 0.087348 \tValidation Loss: 0.095564\n","Epoch: 814 \tTraining Loss: 0.082750 \tValidation Loss: 0.076705\n","Epoch: 815 \tTraining Loss: 0.081288 \tValidation Loss: 0.079468\n","Epoch: 816 \tTraining Loss: 0.078566 \tValidation Loss: 0.076640\n","Epoch: 817 \tTraining Loss: 0.076662 \tValidation Loss: 0.075991\n","Epoch: 818 \tTraining Loss: 0.079505 \tValidation Loss: 0.075516\n","Epoch: 819 \tTraining Loss: 0.075299 \tValidation Loss: 0.080882\n","Epoch: 820 \tTraining Loss: 0.074651 \tValidation Loss: 0.077909\n","Epoch: 821 \tTraining Loss: 0.073665 \tValidation Loss: 0.077752\n","Epoch: 822 \tTraining Loss: 0.073746 \tValidation Loss: 0.079355\n","Epoch: 823 \tTraining Loss: 0.082416 \tValidation Loss: 0.086435\n","Epoch: 824 \tTraining Loss: 0.083320 \tValidation Loss: 0.081015\n","Epoch: 825 \tTraining Loss: 0.076996 \tValidation Loss: 0.076128\n","Epoch: 826 \tTraining Loss: 0.075200 \tValidation Loss: 0.077264\n","Epoch: 827 \tTraining Loss: 0.073010 \tValidation Loss: 0.080753\n","Epoch: 828 \tTraining Loss: 0.076115 \tValidation Loss: 0.073469\n","Epoch: 829 \tTraining Loss: 0.076192 \tValidation Loss: 0.081118\n","Epoch: 830 \tTraining Loss: 0.079726 \tValidation Loss: 0.072525\n","Epoch: 831 \tTraining Loss: 0.076758 \tValidation Loss: 0.081848\n","Epoch: 832 \tTraining Loss: 0.078846 \tValidation Loss: 0.077359\n","Epoch: 833 \tTraining Loss: 0.073946 \tValidation Loss: 0.075851\n","Epoch: 834 \tTraining Loss: 0.075902 \tValidation Loss: 0.076401\n","Epoch: 835 \tTraining Loss: 0.073955 \tValidation Loss: 0.081555\n","Epoch: 836 \tTraining Loss: 0.073351 \tValidation Loss: 0.073932\n","Epoch: 837 \tTraining Loss: 0.073092 \tValidation Loss: 0.079221\n","Epoch: 838 \tTraining Loss: 0.073909 \tValidation Loss: 0.080995\n","Epoch: 839 \tTraining Loss: 0.075737 \tValidation Loss: 0.085233\n","Epoch: 840 \tTraining Loss: 0.074101 \tValidation Loss: 0.076832\n","Epoch: 841 \tTraining Loss: 0.072207 \tValidation Loss: 0.077960\n","Epoch: 842 \tTraining Loss: 0.075308 \tValidation Loss: 0.077394\n","Epoch: 843 \tTraining Loss: 0.073630 \tValidation Loss: 0.072432\n","Epoch: 844 \tTraining Loss: 0.075429 \tValidation Loss: 0.080637\n","Epoch: 845 \tTraining Loss: 0.073209 \tValidation Loss: 0.072433\n","Epoch: 846 \tTraining Loss: 0.073352 \tValidation Loss: 0.084112\n","Epoch: 847 \tTraining Loss: 0.073722 \tValidation Loss: 0.078586\n","Epoch: 848 \tTraining Loss: 0.073592 \tValidation Loss: 0.083443\n","Epoch: 849 \tTraining Loss: 0.072616 \tValidation Loss: 0.082282\n","Epoch: 850 \tTraining Loss: 0.071456 \tValidation Loss: 0.079760\n","Epoch: 851 \tTraining Loss: 0.072625 \tValidation Loss: 0.072987\n","Epoch: 852 \tTraining Loss: 0.071463 \tValidation Loss: 0.077724\n","Epoch: 853 \tTraining Loss: 0.075515 \tValidation Loss: 0.075410\n","Epoch: 854 \tTraining Loss: 0.076108 \tValidation Loss: 0.076137\n","Epoch: 855 \tTraining Loss: 0.073975 \tValidation Loss: 0.075880\n","Epoch: 856 \tTraining Loss: 0.073387 \tValidation Loss: 0.075488\n","Epoch: 857 \tTraining Loss: 0.070853 \tValidation Loss: 0.079026\n","Epoch: 858 \tTraining Loss: 0.073953 \tValidation Loss: 0.079516\n","Epoch: 859 \tTraining Loss: 0.074068 \tValidation Loss: 0.079548\n","Epoch: 860 \tTraining Loss: 0.072294 \tValidation Loss: 0.079969\n","Epoch: 861 \tTraining Loss: 0.071208 \tValidation Loss: 0.076214\n","Epoch: 862 \tTraining Loss: 0.071120 \tValidation Loss: 0.072082\n","Epoch: 863 \tTraining Loss: 0.073533 \tValidation Loss: 0.071245\n","Epoch: 864 \tTraining Loss: 0.071785 \tValidation Loss: 0.077349\n","Epoch: 865 \tTraining Loss: 0.070990 \tValidation Loss: 0.081432\n","Epoch: 866 \tTraining Loss: 0.071667 \tValidation Loss: 0.076937\n","Epoch: 867 \tTraining Loss: 0.070280 \tValidation Loss: 0.074252\n","Epoch: 868 \tTraining Loss: 0.070844 \tValidation Loss: 0.084989\n","Epoch: 869 \tTraining Loss: 0.071920 \tValidation Loss: 0.076532\n","Epoch: 870 \tTraining Loss: 0.071897 \tValidation Loss: 0.082487\n","Epoch: 871 \tTraining Loss: 0.070872 \tValidation Loss: 0.081932\n","Epoch: 872 \tTraining Loss: 0.071313 \tValidation Loss: 0.068022\n","Epoch: 873 \tTraining Loss: 0.074473 \tValidation Loss: 0.077338\n","Epoch: 874 \tTraining Loss: 0.072510 \tValidation Loss: 0.077413\n","Epoch: 875 \tTraining Loss: 0.077311 \tValidation Loss: 0.076306\n","Epoch: 876 \tTraining Loss: 0.071758 \tValidation Loss: 0.080056\n","Epoch: 877 \tTraining Loss: 0.070898 \tValidation Loss: 0.078759\n","Epoch: 878 \tTraining Loss: 0.070457 \tValidation Loss: 0.079415\n","Epoch: 879 \tTraining Loss: 0.071684 \tValidation Loss: 0.086313\n","Epoch: 880 \tTraining Loss: 0.072533 \tValidation Loss: 0.080199\n","Epoch: 881 \tTraining Loss: 0.072360 \tValidation Loss: 0.077223\n","Epoch: 882 \tTraining Loss: 0.071394 \tValidation Loss: 0.080222\n","Epoch: 883 \tTraining Loss: 0.071536 \tValidation Loss: 0.079751\n","Epoch: 884 \tTraining Loss: 0.072397 \tValidation Loss: 0.082466\n","Epoch: 885 \tTraining Loss: 0.074350 \tValidation Loss: 0.075771\n","Epoch: 886 \tTraining Loss: 0.071100 \tValidation Loss: 0.085625\n","Epoch: 887 \tTraining Loss: 0.071880 \tValidation Loss: 0.081488\n","Epoch: 888 \tTraining Loss: 0.072717 \tValidation Loss: 0.074579\n","Epoch: 889 \tTraining Loss: 0.072254 \tValidation Loss: 0.074258\n","Epoch: 890 \tTraining Loss: 0.075149 \tValidation Loss: 0.081752\n","Epoch: 891 \tTraining Loss: 0.071811 \tValidation Loss: 0.081559\n","Epoch: 892 \tTraining Loss: 0.070836 \tValidation Loss: 0.082193\n","Epoch: 893 \tTraining Loss: 0.074158 \tValidation Loss: 0.079083\n","Epoch: 894 \tTraining Loss: 0.073424 \tValidation Loss: 0.090893\n","Epoch: 895 \tTraining Loss: 0.073231 \tValidation Loss: 0.075421\n","Epoch: 896 \tTraining Loss: 0.071913 \tValidation Loss: 0.081216\n","Epoch: 897 \tTraining Loss: 0.072980 \tValidation Loss: 0.081949\n","Epoch: 898 \tTraining Loss: 0.074461 \tValidation Loss: 0.073568\n","Epoch: 899 \tTraining Loss: 0.071203 \tValidation Loss: 0.086137\n","Epoch: 900 \tTraining Loss: 0.070889 \tValidation Loss: 0.071199\n","Epoch: 901 \tTraining Loss: 0.071673 \tValidation Loss: 0.078248\n","Epoch: 902 \tTraining Loss: 0.071551 \tValidation Loss: 0.077040\n","Epoch: 903 \tTraining Loss: 0.074354 \tValidation Loss: 0.071840\n","Epoch: 904 \tTraining Loss: 0.075042 \tValidation Loss: 0.079189\n","Epoch: 905 \tTraining Loss: 0.073062 \tValidation Loss: 0.076901\n","Epoch: 906 \tTraining Loss: 0.070860 \tValidation Loss: 0.084123\n","Epoch: 907 \tTraining Loss: 0.070928 \tValidation Loss: 0.084008\n","Epoch: 908 \tTraining Loss: 0.070709 \tValidation Loss: 0.082003\n","Epoch: 909 \tTraining Loss: 0.070539 \tValidation Loss: 0.082873\n","Epoch: 910 \tTraining Loss: 0.071289 \tValidation Loss: 0.087675\n","Epoch: 911 \tTraining Loss: 0.072237 \tValidation Loss: 0.079289\n","Epoch: 912 \tTraining Loss: 0.072338 \tValidation Loss: 0.081645\n","Epoch: 913 \tTraining Loss: 0.071721 \tValidation Loss: 0.085518\n","Epoch: 914 \tTraining Loss: 0.070317 \tValidation Loss: 0.077138\n","Epoch: 915 \tTraining Loss: 0.070921 \tValidation Loss: 0.082024\n","Epoch: 916 \tTraining Loss: 0.070933 \tValidation Loss: 0.085954\n","Epoch: 917 \tTraining Loss: 0.073103 \tValidation Loss: 0.079411\n","Epoch: 918 \tTraining Loss: 0.073970 \tValidation Loss: 0.086464\n","Epoch: 919 \tTraining Loss: 0.073682 \tValidation Loss: 0.082771\n","Epoch: 920 \tTraining Loss: 0.071048 \tValidation Loss: 0.082875\n","Epoch: 921 \tTraining Loss: 0.071792 \tValidation Loss: 0.077732\n","Epoch: 922 \tTraining Loss: 0.072350 \tValidation Loss: 0.083469\n","Epoch: 923 \tTraining Loss: 0.071055 \tValidation Loss: 0.077039\n","Epoch: 924 \tTraining Loss: 0.071654 \tValidation Loss: 0.081980\n","Epoch: 925 \tTraining Loss: 0.072741 \tValidation Loss: 0.082871\n","Epoch: 926 \tTraining Loss: 0.071702 \tValidation Loss: 0.085362\n","Epoch: 927 \tTraining Loss: 0.070824 \tValidation Loss: 0.086638\n","Epoch: 928 \tTraining Loss: 0.073645 \tValidation Loss: 0.077431\n","Epoch: 929 \tTraining Loss: 0.070834 \tValidation Loss: 0.083730\n","Epoch: 930 \tTraining Loss: 0.070533 \tValidation Loss: 0.089670\n","Epoch: 931 \tTraining Loss: 0.070941 \tValidation Loss: 0.070234\n","Epoch: 932 \tTraining Loss: 0.073335 \tValidation Loss: 0.076142\n","Epoch: 933 \tTraining Loss: 0.071667 \tValidation Loss: 0.080103\n","Epoch: 934 \tTraining Loss: 0.071860 \tValidation Loss: 0.077180\n","Epoch: 935 \tTraining Loss: 0.071011 \tValidation Loss: 0.079671\n","Epoch: 936 \tTraining Loss: 0.071269 \tValidation Loss: 0.075127\n","Epoch: 937 \tTraining Loss: 0.070204 \tValidation Loss: 0.082553\n","Epoch: 938 \tTraining Loss: 0.071831 \tValidation Loss: 0.081957\n","Epoch: 939 \tTraining Loss: 0.075760 \tValidation Loss: 0.080253\n","Epoch: 940 \tTraining Loss: 0.071264 \tValidation Loss: 0.087429\n","Epoch: 941 \tTraining Loss: 0.071619 \tValidation Loss: 0.081131\n","Epoch: 942 \tTraining Loss: 0.070849 \tValidation Loss: 0.087015\n","Epoch: 943 \tTraining Loss: 0.070818 \tValidation Loss: 0.082033\n","Epoch: 944 \tTraining Loss: 0.072356 \tValidation Loss: 0.083210\n","Epoch: 945 \tTraining Loss: 0.073301 \tValidation Loss: 0.080688\n","Epoch: 946 \tTraining Loss: 0.071983 \tValidation Loss: 0.079664\n","Epoch: 947 \tTraining Loss: 0.072004 \tValidation Loss: 0.076955\n","Epoch: 948 \tTraining Loss: 0.071422 \tValidation Loss: 0.085378\n","Epoch: 949 \tTraining Loss: 0.070126 \tValidation Loss: 0.080486\n","Epoch: 950 \tTraining Loss: 0.069397 \tValidation Loss: 0.076206\n","Epoch: 951 \tTraining Loss: 0.070817 \tValidation Loss: 0.083611\n","Epoch: 952 \tTraining Loss: 0.071655 \tValidation Loss: 0.085096\n","Epoch: 953 \tTraining Loss: 0.072584 \tValidation Loss: 0.085586\n","Epoch: 954 \tTraining Loss: 0.073940 \tValidation Loss: 0.077494\n","Epoch: 955 \tTraining Loss: 0.073434 \tValidation Loss: 0.081181\n","Epoch: 956 \tTraining Loss: 0.073304 \tValidation Loss: 0.074676\n","Epoch: 957 \tTraining Loss: 0.074440 \tValidation Loss: 0.079953\n","Epoch: 958 \tTraining Loss: 0.071680 \tValidation Loss: 0.076060\n","Epoch: 959 \tTraining Loss: 0.071180 \tValidation Loss: 0.089179\n","Epoch: 960 \tTraining Loss: 0.071047 \tValidation Loss: 0.089160\n","Epoch: 961 \tTraining Loss: 0.073574 \tValidation Loss: 0.080511\n","Epoch: 962 \tTraining Loss: 0.072907 \tValidation Loss: 0.084551\n","Epoch: 963 \tTraining Loss: 0.073499 \tValidation Loss: 0.075179\n","Epoch: 964 \tTraining Loss: 0.072520 \tValidation Loss: 0.088237\n","Epoch: 965 \tTraining Loss: 0.072172 \tValidation Loss: 0.075001\n","Epoch: 966 \tTraining Loss: 0.074964 \tValidation Loss: 0.078816\n","Epoch: 967 \tTraining Loss: 0.072276 \tValidation Loss: 0.080156\n","Epoch: 968 \tTraining Loss: 0.072706 \tValidation Loss: 0.084852\n","Epoch: 969 \tTraining Loss: 0.071740 \tValidation Loss: 0.073032\n","Epoch: 970 \tTraining Loss: 0.070613 \tValidation Loss: 0.086334\n","Epoch: 971 \tTraining Loss: 0.070024 \tValidation Loss: 0.082426\n","Epoch: 972 \tTraining Loss: 0.073629 \tValidation Loss: 0.084048\n","Epoch: 973 \tTraining Loss: 0.070963 \tValidation Loss: 0.078551\n","Epoch: 974 \tTraining Loss: 0.072228 \tValidation Loss: 0.082832\n","Epoch: 975 \tTraining Loss: 0.072530 \tValidation Loss: 0.079813\n","Epoch: 976 \tTraining Loss: 0.070621 \tValidation Loss: 0.077113\n","Epoch: 977 \tTraining Loss: 0.071258 \tValidation Loss: 0.078413\n","Epoch: 978 \tTraining Loss: 0.070491 \tValidation Loss: 0.083976\n","Epoch: 979 \tTraining Loss: 0.072524 \tValidation Loss: 0.076059\n","Epoch: 980 \tTraining Loss: 0.072264 \tValidation Loss: 0.084225\n","Epoch: 981 \tTraining Loss: 0.071231 \tValidation Loss: 0.079794\n","Epoch: 982 \tTraining Loss: 0.070987 \tValidation Loss: 0.087882\n","Epoch: 983 \tTraining Loss: 0.080228 \tValidation Loss: 0.076324\n","Epoch: 984 \tTraining Loss: 0.076328 \tValidation Loss: 0.078926\n","Epoch: 985 \tTraining Loss: 0.072761 \tValidation Loss: 0.078694\n","Epoch: 986 \tTraining Loss: 0.073226 \tValidation Loss: 0.081351\n","Epoch: 987 \tTraining Loss: 0.070857 \tValidation Loss: 0.085618\n","Epoch: 988 \tTraining Loss: 0.072548 \tValidation Loss: 0.084931\n","Epoch: 989 \tTraining Loss: 0.071271 \tValidation Loss: 0.092220\n","Epoch: 990 \tTraining Loss: 0.071705 \tValidation Loss: 0.084744\n","Epoch: 991 \tTraining Loss: 0.070902 \tValidation Loss: 0.082524\n","Epoch: 992 \tTraining Loss: 0.070686 \tValidation Loss: 0.082983\n","Epoch: 993 \tTraining Loss: 0.071793 \tValidation Loss: 0.083647\n","Epoch: 994 \tTraining Loss: 0.071421 \tValidation Loss: 0.087365\n","Epoch: 995 \tTraining Loss: 0.070140 \tValidation Loss: 0.077597\n","Epoch: 996 \tTraining Loss: 0.070292 \tValidation Loss: 0.088343\n","Epoch: 997 \tTraining Loss: 0.070975 \tValidation Loss: 0.077930\n","Epoch: 998 \tTraining Loss: 0.073593 \tValidation Loss: 0.081651\n","Epoch: 999 \tTraining Loss: 0.071282 \tValidation Loss: 0.083395\n","Epoch: 1000 \tTraining Loss: 0.072442 \tValidation Loss: 0.085885\n","Total time: 1531.486377954483\n"]}],"source":["print(\"[INFO] training the network...\")\n","startTime = time.time()\n","valid_loss_min = np.Inf\n","valid_error_min = np.Inf\n","\n","for e in range(0, EPOCHS):\n","\t# set the model in training mode\n","\t# monitor losses\n","\ttrain_loss = 0\n","\tvalid_loss = 0\n","\tvalid_error = 0\n","\t\n","\t###################\n","\t# train the model #\n","\t###################\n","\tmodel.train() # prep model for training\n","\tfor data,label in train_loader:\n","\t\tdata, label = data.to(device), label.to(device)\n","\t\t# clear the gradients of all optimized variables\n","\t\toptimizer.zero_grad()\n","\t\t# forward pass: compute predicted outputs by passing inputs to the model\n","\t\toutput = model(data.float())\n","\t\t# calculate the loss\n","\t\t#print(output,label.float() )\n","\t\tloss = criterion(output,label.float())\n","\t\t# backward pass: compute gradient of the loss with respect to model parameters\n","\t\tloss.backward()\n","\t\t# perform a single optimization step (parameter update)\n","\t\toptimizer.step()\n","\t\t# update running training loss\n","\t\ttrain_loss += loss.item() * data.size(0)\n","\t\t#print(train_loss, data.size(0), loss.item())\n","\t\t\t\t\n","\t######################    \n","\t# validate the model #\n","\t######################\n","\tmodel.eval()  # prep model for evaluation\n","\tfor data,label in valid_loader:\n","\t\t# forward pass: compute predicted outputs by passing inputs to the model\n","\t\tdata, label = data.to(device), label.to(device)\n","\t\toutput = model(data.float())\n","\t\t# calculate the loss\n","\t\tloss = criterion(output,label)\n","\t\t# update running validation loss \n","\t\tvalid_loss += loss.item() * data.size(0)\n","\t\tvalid_error += torch.sum(torch.div(torch.abs(output-label), torch.abs(label)))\n","\t\n","\t# print training/validation statistics \n","\t# calculate average loss over an epoch\n","\ttrain_loss = train_loss / len(train_loader.sampler)\n","\tvalid_loss = valid_loss / len(valid_loader.sampler)\n","\tvalid_error = valid_error/ len(valid_loader.sampler)\n","\t\n","\tprint('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","\t\te+1, \n","\t\ttrain_loss,\n","\t\tvalid_loss\n","\t\t))\n","\t\n","\t# save model if validation loss has decreased\n","\t# if valid_loss <= valid_loss_min:\n","\t# \tprint('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","\t# \tvalid_loss_min,\n","\t# \tvalid_loss))\n","\t# \ttorch.save(model.state_dict(), '/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/pytorch/models/perceptron.pt')\n","\t# \tvalid_loss_min = valid_loss\n","\n","\tif valid_error <= valid_error_min:\n","\t\tprint('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","\t\tvalid_error_min,\n","\t\tvalid_error))\n","\t\ttorch.save(model.state_dict(), '/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/pytorch/models/perceptron.pt')\n","\t\tvalid_error_min = valid_error\n","\n","print('Total time:', time.time()- startTime)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1667333699029,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"QQQaL5Djq-Zw"},"outputs":[],"source":["# initialize lists to monitor test loss and accuracy\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Gdrive/startup/Novus Sentry/src/pytorch/models/perceptron.pt'))\n","test_loss = 0.0\n","test_error = 0.0\n","\n","model.eval() # prep model for evaluation\n","for data, target in test_loader:\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    data, target = data.to(device), target.to(device)\n","    output = model(data.float())\n","    # calculate the loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    test_error += torch.sum(torch.div(torch.abs(output-target), torch.abs(target)))\n","\n","# calculate and print avg test loss\n","test_loss = test_loss/len(test_loader.sampler)\n","test_error = test_error/len(test_loader.sampler)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Test Error: {:.6f}\\n'.format(test_error))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1667333699029,"user":{"displayName":"Janak Agrawal","userId":"14377288670060290274"},"user_tz":-330},"id":"_a1UvVEut5JB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.8 ('novus': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"93326684faddb4973a2f243ae775d197514564119db8c67ae7c7306dee8a5cbc"}}},"nbformat":4,"nbformat_minor":0}
